import string
import re
import camel_tools.utils.dediac as camel_utils_dediac
import camel_tools.utils.normalize as camel_utils_normalize
import camel_tools.tokenizers.word as camel_utils_tokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from utils import UNICODE_EMO

# from camel_tools.morphology.analyzer import Analyzer
# from camel_tools.morphology.database import MorphologyDB

# from farasa.segmenter import FarasaSegmenter
# from farasa.stemmer import FarasaStemmer

import nltk
import arabicstopwords.arabicstopwords as stp

# SPECIAL TOKENS:
# <LINK> <NUM> <Mt> <LF> for links, numbers, mentions, line feed
# ENGLISH Text is kept as is
# HASHTAGS are repeated 3 times
# TODO: Emojis

class Preprocess:

    # farasa_seg = FarasaSegmenter(interactive=True)
    # farasa_stm = FarasaStemmer(interactive=True)

    nltk_arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))
    arabicstopwords = set(stp.stopwords_list())

    def __init__(self, INCLUDE_EMOJIS=True, HASH_FREQ = 3, LEMMATIZOR = 'camel') -> None:
        print("Emojis: ", INCLUDE_EMOJIS)
        print("Lemmatizor: ", LEMMATIZOR)
        self.INCLUDE_EMOJIS = INCLUDE_EMOJIS
        self.LEMMATIZOR = LEMMATIZOR
        self.HASH_FREQ = HASH_FREQ
        self.mle = MLEDisambiguator.pretrained()
        # This is done on tokens, remove empty tokens
        self.STOPWORDS = self.arabicstopwords.union(['', ' '])

    # string -> dediacritized string
    # print(dediac("Ù…ÙŠØ³Ø¨Øª"));
    def dediac(self, text):
        # Can also do using regex
        return camel_utils_dediac.dediac_ar(text)

    # replace:
        # 1. links with <LINK>
        # 2. numbers with <NUM>
        # 3. Mentions (@USER) with <Mt>
    def tokens(self, text):
        ret = text
        # all links are https://t.co/...
        # add extra space to avoid joining words
        ret = re.sub(r'http\S+', ' <LINK> ', ret)
        ret = re.sub(r'\d+', ' <NUM> ', ret)
        ret = re.sub(r'\S*@\S*', ' <Mt> ', ret)
        ret = re.sub('<LF>', ' <LF> ', ret)
        return ret

    def convert_emojis_to_meaning(self, text):
        # emojis unicode are '\U000[0-9A-F]{5}'
        # emojis = re.findall(r'\\U000[0-9A-F]{5}', text)
        #
        # map the emoji unicode into its english word
        # remove : , from the english word
        # the emoji word is represented as a single token even if it consists of multiple words
        # i.e ğŸŒ¹ "red_flower" not "red flower"
        # add space before and after the emoji word, since usually emojis are written close to each other without spaces.
        # i.e. want ğŸ”¥ğŸ”¥ğŸ”¥ -> "fire fire fire" not "firefirefire"
        return "".join([' <'+UNICODE_EMO[c]+'> ' if c in UNICODE_EMO else c for c in text])

    # string -> remove punctuation
    def remove_punctuation(self, text):
        punc = '''!()-[]{};:'"\,<>./?@#$%^&*_~''' + '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€Â«Â»'''
        punc += '''â‰'''
        # keep HashTags ?
        punc = punc.replace('#', '')
        punc = punc.replace('_', '')
        # keep tokens: <...>
        punc = punc.replace('<', '')
        punc = punc.replace('>', '')
        # replace punc with space
        return text.translate(str.maketrans(punc, ' ' * len(punc)))

    # replacing: Ø£ Ø¥ Ø¢ with Ø§
    # replacing: Ø© with Ù‡
    # replacing: ÙŠ with Ù‰
    # Maybe add Ø¦ Ø¤ ?
    def normalize(self, text):
        ret = text
        ret = camel_utils_normalize.normalize_alef_ar(ret)
        ret = camel_utils_normalize.normalize_teh_marbuta_ar(ret)
        ret = camel_utils_normalize.normalize_alef_maksura_ar(ret)
        # replace ØªØ¨Ø³Ù… with Øª Ø¨ Ø³ Ù… ?
        # ret = camel_utils_normalize.normalize_unicode(ret)
        return ret

    # English words?         --> kept intact
    # <LINK> <NUM> <Mt> ?    --> made into 3 tokens
    def tokenizer(self, text):
        # TODO: better handling of <LF> ??
        # TODO: check camel_tools.tokenizers.morphological.MorphologicalTokenizer

        # extract < .. >
        e1 = re.findall(r'<\S+>', text)
        # remove < .. >
        text = re.sub(r'<\S+>', '', text)
        # extact # .. _ .. _ ..
        e2 = re.findall(r'#(\S+)', text)
        # split on _
        e2 = [tg.split('_') for tg in e2]
        # flatten
        e2 = [item for sublist in e2 for item in sublist if item != '' and item != ' ']
        # remove # .. _ .. _ ..
        text = re.sub(r'#\S+', '', text)

        ret = camel_utils_tokenizer.simple_word_tokenize(text)
        # add extracted!
        ret += e1
        # repeat hashtags for heavier weight!
        for tg in e2:
            ret += [tg] *self.HASH_FREQ

        return ret

    def camel_lemmatize(self, tokenized_text):
        disambig = self.mle.disambiguate(tokenized_text)
        lemmas = [d.analyses[0].analysis['lex'] for d in disambig]
        return lemmas

    # Farasapy is bad
    # This doesn't give same result as their API!
    def farasapy_lemmatize(self, text):
        ret = self.farasa_stm.stem(text)
        return self.tokenizer(ret)

    def lemmatize(self, text, method='camel'):
        if method == 'camel':
            return self.camel_lemmatize(text)
        elif method == 'farasapy':
            return self.farasapy_lemmatize(text)
        else:
            raise Exception('Invalid method')

    # After lemmas?
    def remove_stopwords(self, tokenized_text):
        ret = [tk for tk in tokenized_text if tk not in self.STOPWORDS]
        return ret

    # do:
    #    1. dediac
    #    2. replace links, numbers, mentions
    #    3. convert emojis to meaning
    #    4. remove punctuation
    #    5. normalize
    #    6. tokenize & lemmatize
    #    7. remove stopwords
    def do_all(self, text):
        if self.INCLUDE_EMOJIS:
            ret = self.normalize(self.remove_punctuation(self.convert_emojis_to_meaning(self.tokens(self.dediac(text)))))
        else:
            ret = self.normalize(self.remove_punctuation(self.tokens(self.dediac(text))))
        #
        if self.LEMMATIZOR == 'camel':
            return self.remove_stopwords(self.camel_lemmatize(self.tokenizer(ret)))
        elif self.LEMMATIZOR == 'farasapy':
            return self.remove_stopwords(self.farasapy_lemmatize(ret))
        else:
            raise Exception('Invalid lemmatizor')

# open file

# p = Preprocess(INCLUDE_EMOJIS=True)

# f = open("output.txt", "w")
# print("NATIVE:", p.farasapy_lemmatize('ÙŠÙØ´Ø§Ø± Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙŠØªØ­Ø¯Ø«Ù‡Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 422 Ù…Ù„ÙŠÙˆÙ† Ù†Ø³Ù…Ø© ÙˆÙŠØªÙˆØ²Ø¹ Ù…ØªØ­Ø¯Ø«ÙˆÙ‡Ø§ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø¨Ø§Ø³Ù… Ø§Ù„ÙˆØ·Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø© Ù…Ø«Ù„ Ø§Ù„Ø£Ù‡ÙˆØ§Ø² ÙˆØªØ±ÙƒÙŠØ§ ÙˆØªØ´Ø§Ø¯ ÙˆØ§Ù„Ø³Ù†ØºØ§Ù„ ÙˆØ¥Ø±ÙŠØªØ±ÙŠØ§ ÙˆØºÙŠØ±Ù‡Ø§. ÙˆÙ‡ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ù…Ù† Ù„ØºØ§Øª Ù…Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„Ø³Øª.'), file=f)
# print("ALL: ", p.do_all('ÙŠÙØ´Ø§Ø± Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙŠØªØ­Ø¯Ø«Ù‡Ø§ Ø£ÙƒØ«Ø± Ù…Ù† 422 Ù…Ù„ÙŠÙˆÙ† Ù†Ø³Ù…Ø© ÙˆÙŠØªÙˆØ²Ø¹ Ù…ØªØ­Ø¯Ø«ÙˆÙ‡Ø§ ÙÙŠ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ø¨Ø§Ø³Ù… Ø§Ù„ÙˆØ·Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„Ù…Ø¬Ø§ÙˆØ±Ø© Ù…Ø«Ù„ Ø§Ù„Ø£Ù‡ÙˆØ§Ø² ÙˆØªØ±ÙƒÙŠØ§ ÙˆØªØ´Ø§Ø¯ ÙˆØ§Ù„Ø³Ù†ØºØ§Ù„ ÙˆØ¥Ø±ÙŠØªØ±ÙŠØ§ ÙˆØºÙŠØ±Ù‡Ø§. ÙˆÙ‡ÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ù…Ù† Ù„ØºØ§Øª Ù…Ù†Ø¸Ù…Ø© Ø§Ù„Ø£Ù…Ù… Ø§Ù„Ù…ØªØ­Ø¯Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„Ø³Øª.'), file=f)

# print(camel_utils_normalize.normalize_unicode('Ù…ÙŠØ³Ø¨Øª'))

# print(p.dediac('asd'))

# print(p.do_all('''Ø¨ÙŠÙ„ ØºÙŠØªØ³ ÙŠØªÙ„Ù‚Ù‰ Ù„Ù‚Ø§Ø­ #ÙƒÙˆÙÙŠØ¯19 Ù…Ù† ØºÙŠØ± ØªØµÙˆÙŠØ± Ø§Ù„Ø§Ø¨Ø±Ø© Ùˆ Ù„Ø§ Ø§Ù„Ø³ÙŠØ±Ù†Ø¬Ø© Ùˆ Ù„Ø§ Ø§Ù„Ø¯ÙˆØ§Ø¡ Ùˆ Ù„Ø§Ø¨Ø³ Ø¨ÙˆÙ„Ùˆ ØµÙŠÙÙŠ ÙÙŠ Ø¹Ø² Ø§Ù„Ø´ØªØ§Ø¡ Ùˆ ÙŠÙ‚ÙˆÙ„ Ø§Ù† Ø¥Ø­Ø¯Ù‰ Ù…Ø²Ø§ÙŠØ§ Ø¹Ù…Ø± Ø§Ù„ 65 Ø¹Ø§Ù…Ù‹Ø§ Ù‡ÙŠ Ø§Ù†Ù‡ Ù…Ø¤Ù‡Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø­ ... ÙŠØ¹Ù†Ù‰ Ù…Ø§ ÙƒØ§Ù† ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù„Ù‚Ø§Ø­ Ù„Ùˆ ÙƒØ§Ù† Ø¹Ù…Ø±Ù‡ Ø§ØµØºØ± Ù…Ù† 65 ğŸ¤” https://t.co/QQKFFUNwBn,celebrity,1ÙˆØ²ÙŠØ± Ø§Ù„ØµØ­Ø© Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ… ÙˆØªØ­Ø¯ÙŠØ¯Ø§ Ù‡Ù„Ø£ Ø¨Ù…Ø¤ØªÙ…Ø±ÙˆØ§ Ø§Ù„ØµØ­ÙÙŠ ÙƒØ§Ù† Ù…Ø§ Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ÙˆØ§.<LF>Ø¨Ø³ Ø§Ù†ÙˆØ§ Ø¨ÙØ§ÙŠØ²Ø± Ù‡Ùˆ Ø§Ù„Ù„Ù‚Ø§Ø­ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø¢Ù…Ù† Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ù„Ø£ Ø­Ø¨ÙŠØ¨ÙŠ Ù‡Ø§ÙŠ ÙÙŠÙ‡Ø§ Ù†ÙØ§Ù‚ ÙˆØ§Ø¶Ø­.<LF>Ø¹ÙÙˆØ§ ÙŠØ¹Ù†ÙŠ.<LF>Ù…Ø´ Ø¥Ù†ÙˆØ§ Ù…Ø¬Ø¨ÙˆØ±ÙŠÙ† ÙÙŠÙ‡ ÙˆÙ…Ø§ Ù‚Ø§Ø¯Ø± ØªØ¬Ø¨Ø± Ø§Ù„Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ø§Ø±Ù‚Ø© ØªØ¯ÙØ¹ Ù…ØµØ§Ø±ÙŠ ØªØ¬ÙŠØ¨ Ù„Ù‚Ø§Ø­ ØªØ§Ù†ÙŠ.<LF>ÙŠÙ„Ø§ Ø¨Ø§ÙŠ,info_news,1Ù‚ÙˆÙ„ÙƒÙ†  Ø±Ø­ ÙŠÙƒÙˆÙ†Ùˆ Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø¨ Ù„Ø¨Ù†Ø§Ù† Ù„Ù…Ø§ ÙŠÙˆØµÙ„ Ø§Ù„Ù„Ù‚Ø§Ø­ØŸ<LF>Ø£Ù„ÙÙŠ Ø¬Ø±Ø¹Ø© Ù…Ù† Ù„Ù‚Ø§Ø­ #ÙƒÙˆØ±ÙˆÙ†Ø§ Ù„Ù„ØªÙ„Ù ÙÙŠ Ù…Ø³ØªØ´ÙÙ‰ Ø¨ÙˆØ³Ø·Ù†ØŒÙ„Ø£Ù† Ø¹Ø§Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§ÙØ© ÙˆÙ‚Ù Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø®Ø·Ø£ Ø¹Ù…Ù„ Ø§Ù„Ø«Ù„Ø§Ø¬Ø© Ø§Ù„ØªÙŠ Ø­ÙØ¸Øª Ø¨Ù‡Ø§.<LF>#Ù„Ù‚Ø§Ø­_ÙƒÙˆØ±ÙˆÙ†Ø§,info_news,1'''))

# print(p.do_all('''Ù…Ø§Øª 23 Ø´Ø®Øµ ÙÙŠ Ø§Ù„Ù†Ø±ÙˆÙŠØ¬ Ø¨Ø¹Ø¯ ØªÙ„Ù‚ÙŠ Ù„Ù‚Ø§Ø­  #COVID19..<LF>Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ Ø¨ÙˆØ³Øª ğŸ¤¦ https://t.co/g51cmWeoVL,info_news,-1''', lemmatizor='farasapy'))
# print(p.do_all('''Ù…Ø§Øª 23 Ø´Ø®Øµ ÙÙŠ Ø§Ù„Ù†Ø±ÙˆÙŠØ¬ Ø¨Ø¹Ø¯ ØªÙ„Ù‚ÙŠ Ù„Ù‚Ø§Ø­  #COVID19..<LF>Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ Ø¨ÙˆØ³Øª ğŸ¤¦ https://t.co/g51cmWeoVL,info_news,-1''', lemmatizor='camel'))

# print(p.convert_emojis_to_meaning("ğŸ˜‚ğŸ˜¶ğŸ˜¶ğŸ¤”â¤ğŸ‘ğŸ»ğŸ˜"));
# print("empty test");

