import string
import re
import camel_tools.utils.dediac as camel_utils_dediac
import camel_tools.utils.normalize as camel_utils_normalize
import camel_tools.tokenizers.word as camel_utils_tokenizer
from camel_tools.disambig.mle import MLEDisambiguator
from utils import UNICODE_EMO

# from camel_tools.morphology.analyzer import Analyzer
# from camel_tools.morphology.database import MorphologyDB

# from farasa.segmenter import FarasaSegmenter
# from farasa.stemmer import FarasaStemmer

import nltk
import arabicstopwords.arabicstopwords as stp

# SPECIAL TOKENS:
# <LINK> <NUM> <Mt> <LF> for links, numbers, mentions, line feed
# ENGLISH Text is kept as is
# HASHTAGS are repeated 3 times
# TODO: Emojis

class Preprocess:

    # farasa_seg = FarasaSegmenter(interactive=True)
    # farasa_stm = FarasaStemmer(interactive=True)

    nltk_arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))
    arabicstopwords = set(stp.stopwords_list())

    def __init__(self, INCLUDE_EMOJIS=True, HASH_FREQ=2, LEMMATIZOR='camel') -> None:
        print("Emojis: ", INCLUDE_EMOJIS)
        print("Lemmatizor: ", LEMMATIZOR)
        self.INCLUDE_EMOJIS = INCLUDE_EMOJIS
        self.LEMMATIZOR = LEMMATIZOR
        self.HASH_FREQ = HASH_FREQ
        self.mle = MLEDisambiguator.pretrained()
        # Empty tokens as stop words
        self.STOPWORDS = self.arabicstopwords.union(['', ' '])

    # string -> dediacritized string
    # print(dediac("ููุณุจุช"));
    def dediac(self, text):
        # Can also do using regex
        return camel_utils_dediac.dediac_ar(text)

    # replace:
        # 1. links with <LINK>
        # 2. numbers with <NUM>
        # 3. Mentions (@USER) with <Mt>
    def tokens(self, text):
        ret = text
        # all links are https://t.co/...
        # add extra space to avoid joining words
        ret = re.sub(r'http\S+', ' <LINK> ', ret)
        ret = re.sub(r'\d+', ' <NUM> ', ret)
        ret = re.sub(r'\S*@\S*', ' <Mt> ', ret)
        ret = re.sub('<LF>', ' <LF> ', ret)
        return ret

    def convert_emojis_to_meaning(self, text):
        # emojis unicode are '\U000[0-9A-F]{5}'
        # emojis = re.findall(r'\\U000[0-9A-F]{5}', text)
        #
        # map the emoji unicode into its english word
        # remove : , from the english word
        # the emoji word is represented as a single token even if it consists of multiple words
        # i.e ๐น "red_flower" not "red flower"
        # add space before and after the emoji word, since usually emojis are written close to each other without spaces.
        # i.e. want ๐ฅ๐ฅ๐ฅ -> "fire fire fire" not "firefirefire"
        return "".join([' <'+UNICODE_EMO[c]+'> ' if c in UNICODE_EMO else c for c in text])

    # string -> remove punctuation
    def remove_punctuation(self, text):
        punc = '''!()-[]{};:'"\,<>./?@#$%^&*_~''' + '''`รทรุ<>_()*&^%][ูุ/:"ุ.,'{}~ยฆ+|!โโฆโโูยซยป'''
        punc += '''โ'''

        # keep HashTags 
        hashtags = re.findall(r'#\S+', text)
        text = re.sub(r'#\S+', '', text)
        # keep tokens: <...>
        tokens = re.findall(r'<\S+>', text)
        text = re.sub(r'<\S+>', '', text)

        # replace punc with space
        text = text.translate(str.maketrans(punc, ' ' * len(punc)))
        # re-add tokens
        return text +' '.join(tokens) +' '.join(hashtags)

    # replacing: ุฃ ุฅ ุข with ุง
    # replacing: ุฉ with ู
    # replacing: ู with ู
    # Maybe add ุฆ ุค ?
    def normalize(self, text):
        ret = text
        ret = camel_utils_normalize.normalize_alef_ar(ret)
        ret = camel_utils_normalize.normalize_teh_marbuta_ar(ret)
        ret = camel_utils_normalize.normalize_alef_maksura_ar(ret)
        # replace ุชุจุณู with ุช ุจ ุณ ู ?
        # ret = camel_utils_normalize.normalize_unicode(ret)
        return ret

    # English words?         --> kept intact
    # <LINK> <NUM> <Mt> ?    --> made into 3 tokens
    def tokenizer(self, text):
        # TODO: better handling of <LF> ??
        # TODO: check camel_tools.tokenizers.morphological.MorphologicalTokenizer

        # extract < .. >
        e1 = re.findall(r'<\S+>', text)
        # remove < .. >
        text = re.sub(r'<\S+>', '', text)
        # extact # .. _ .. _ ..
        e2 = re.findall(r'#(\S+)', text)
        # split on _
        e2 = [tg.split('_') for tg in e2]
        # flatten
        e2 = [item for sublist in e2 for item in sublist if item != '' and item != ' ']
        # remove # .. _ .. _ ..
        text = re.sub(r'#\S+', '', text)

        ret = camel_utils_tokenizer.simple_word_tokenize(text)
        # add extracted!
        ret += e1
        # repeat hashtags for heavier weight!
        for tg in e2:
            ret += [tg] *self.HASH_FREQ

        return ret

    def camel_lemmatize(self, tokenized_text):
        disambig = self.mle.disambiguate(tokenized_text)
        lemmas = [d.analyses[0].analysis['lex'] for d in disambig]
        return lemmas

    # Farasapy is bad
    # This doesn't give same result as their API!
    def farasapy_lemmatize(self, text):
        ret = self.farasa_stm.stem(text)
        return self.tokenizer(ret)

    def lemmatize(self, text, method='camel'):
        if method == 'camel':
            return self.camel_lemmatize(text)
        elif method == 'farasapy':
            return self.farasapy_lemmatize(text)
        else:
            raise Exception('Invalid method')

    # After lemmas?
    def remove_stopwords(self, tokenized_text):
        ret = [tk for tk in tokenized_text if self.dediac(tk) not in self.STOPWORDS]
        return ret

    # do:
    #    1. dediac
    #    2. replace links, numbers, mentions
    #    3. convert emojis to meaning
    #    4. remove punctuation
    #    5. normalize
    #    6. tokenize & lemmatize
    #    7. remove stopwords
    def do_all(self, text):
        if self.INCLUDE_EMOJIS:
            ret = self.normalize(self.remove_punctuation(self.convert_emojis_to_meaning(self.tokens(self.dediac(text)))))
        else:
            ret = self.normalize(self.remove_punctuation(self.tokens(self.dediac(text))))
        #
        # print('tokens:', ret)
        if self.LEMMATIZOR == 'camel':
            return self.remove_stopwords(self.camel_lemmatize(self.tokenizer(ret)))
        elif self.LEMMATIZOR == 'farasapy':
            return self.remove_stopwords(self.farasapy_lemmatize(ret))
        else:
            raise Exception('Invalid lemmatizor')

# open file

# p = Preprocess()
# x = '''
# ููุงุญ #ูุงูุฒุฑ/ุจูููุชูู https://t.co/LHlDwaLhby,info_news,1

# train   ุฎุจุฑุงุก ุตุญุฉ ุตููููู ูุฏุนูู ุฅูู ุชุนููู ุงุณุชุฎุฏุงู ููุงุญ ูุงูุฒุฑ/ุจูููุชูู"" ู""ููุฏูุฑูุง""",info_news,1

# ุฎุจุฑ ุฌูุฏ ุนู ูุนุงููู ููุงุญ ูุงูุฒุฑ/ุจูููุชูู ุถุฏ ุงูุชุญููุงุช ุงูุฌูููู ุงูุฌุฏูุฏู ุงูุชู ุญุฏุซุช ูููุฑูุณ ููุฑููุง ูุฌุนูุชู ุงูุซุฑ ูุฏุฑู ุนูู ุงูุงูุชุดุงุฑ ูุฎุตูุตุง ุงูุทูุฑู ุงูุฌุฏูุฏู ุงูุชู ุชู ุฑุตุฏูุง ูุงูุชู ุชุบูุฑ ุงุฌุฒุงุก ูู ุงูุจุฑูุชูู ุงูุดููู. <LF> https://t.co/cYcEAfcNp5,info_news,1

# ุจุนุฏ ูููุงุช ุงููุฑููุฌ.. ุฎุจุฑุงุก ุตุญุฉ ุตููููู ูุฏุนูู ุฅูู ุชุนููู ุงุณุชุฎุฏุงู ููุงุญ โูุงูุฒุฑ/ุจูููุชููโ ูโููุฏูุฑูุงโ https://t.co/9whK0H6dTQ,info_news,-1

# ุซุงูู ููุงุญ ูุญุตู ุนูู ุชุฑุฎูุต ูู ููุงูุฉ ุงูุฃุฏููุฉ ุงูุฃูุฑูุจูุฉุ ุจุนุฏ ุงูุณูุงุญ ุจุงุณุชุฎุฏุงู ููุงุญ ูุงูุฒุฑ/ุจูููุชูู ูู ุฏูู ุงูุงุชุญุงุฏ ุงูู27 https://t.co/EID7q81aMx #ุงูุนุฑุจูุฉ,info_news,1"""
# '''
# x = 'ุชุณุงุคูุงุช ุขุฎุฑ ููู <LF>ุงุฐุง ูุง ููุงูุง ููุงุญ ุงูููุฑููุง ...ุจุฒูุฏููู ูู ุุ ๐<LF>#ูุจู'
# print(p.do_all(x))

# f = open("output.txt", "w")
# print("NATIVE:", p.farasapy_lemmatize('ููุดุงุฑ ุฅูู ุฃู ุงููุบุฉ ุงูุนุฑุจูุฉ ูุชุญุฏุซูุง ุฃูุซุฑ ูู 422 ููููู ูุณูุฉ ููุชูุฒุน ูุชุญุฏุซููุง ูู ุงูููุทูุฉ ุงููุนุฑููุฉ ุจุงุณู ุงููุทู ุงูุนุฑุจู ุจุงูุฅุถุงูุฉ ุฅูู ุงูุนุฏูุฏ ูู ุงูููุงุทู ุงูุฃุฎุฑู ุงููุฌุงูุฑุฉ ูุซู ุงูุฃููุงุฒ ูุชุฑููุง ูุชุดุงุฏ ูุงูุณูุบุงู ูุฅุฑูุชุฑูุง ูุบูุฑูุง. ููู ุงููุบุฉ ุงูุฑุงุจุนุฉ ูู ูุบุงุช ููุธูุฉ ุงูุฃูู ุงููุชุญุฏุฉ ุงูุฑุณููุฉ ุงูุณุช.'), file=f)
# print("ALL: ", p.do_all('ููุดุงุฑ ุฅูู ุฃู ุงููุบุฉ ุงูุนุฑุจูุฉ ูุชุญุฏุซูุง ุฃูุซุฑ ูู 422 ููููู ูุณูุฉ ููุชูุฒุน ูุชุญุฏุซููุง ูู ุงูููุทูุฉ ุงููุนุฑููุฉ ุจุงุณู ุงููุทู ุงูุนุฑุจู ุจุงูุฅุถุงูุฉ ุฅูู ุงูุนุฏูุฏ ูู ุงูููุงุทู ุงูุฃุฎุฑู ุงููุฌุงูุฑุฉ ูุซู ุงูุฃููุงุฒ ูุชุฑููุง ูุชุดุงุฏ ูุงูุณูุบุงู ูุฅุฑูุชุฑูุง ูุบูุฑูุง. ููู ุงููุบุฉ ุงูุฑุงุจุนุฉ ูู ูุบุงุช ููุธูุฉ ุงูุฃูู ุงููุชุญุฏุฉ ุงูุฑุณููุฉ ุงูุณุช.'), file=f)

# print(camel_utils_normalize.normalize_unicode('ููุณุจุช'))

# print(p.dediac('asd'))

# print(p.do_all('''ุจูู ุบูุชุณ ูุชููู ููุงุญ #ููููุฏ19 ูู ุบูุฑ ุชุตููุฑ ุงูุงุจุฑุฉ ู ูุง ุงูุณูุฑูุฌุฉ ู ูุง ุงูุฏูุงุก ู ูุงุจุณ ุจููู ุตููู ูู ุนุฒ ุงูุดุชุงุก ู ูููู ุงู ุฅุญุฏู ูุฒุงูุง ุนูุฑ ุงู 65 ุนุงููุง ูู ุงูู ูุคูู ููุญุตูู ุนูู ุงูููุงุญ ... ูุนูู ูุง ูุงู ูุญุชุงุฌ ุงูููุงุญ ูู ูุงู ุนูุฑู ุงุตุบุฑ ูู 65 ๐ค https://t.co/QQKFFUNwBn,celebrity,1ูุฒูุฑ ุงูุตุญุฉ ูุญุฏ ุงูููู ูุชุญุฏูุฏุง ููุฃ ุจูุคุชูุฑูุง ุงูุตุญูู ูุงู ูุง ุนูุฏู ูุดููุฉ ูุนูุง.<LF>ุจุณ ุงููุง ุจูุงูุฒุฑ ูู ุงูููุงุญ ุงููุญูุฏ ุงูุขูู ุจุงูุนุงูู ูุฃ ุญุจูุจู ูุงู ูููุง ููุงู ูุงุถุญ.<LF>ุนููุง ูุนูู.<LF>ูุด ุฅููุง ูุฌุจูุฑูู ููู ููุง ูุงุฏุฑ ุชุฌุจุฑ ุงูุฏููุฉ ุงููุงุฑูุฉ ุชุฏูุน ูุตุงุฑู ุชุฌูุจ ููุงุญ ุชุงูู.<LF>ููุง ุจุงู,info_news,1ููููู  ุฑุญ ููููู ุงุฏ ุงููุณุคูููุฉ ุจ ูุจูุงู ููุง ููุตู ุงูููุงุญุ<LF>ุฃููู ุฌุฑุนุฉ ูู ููุงุญ #ููุฑููุง ููุชูู ูู ูุณุชุดูู ุจูุณุทูุูุฃู ุนุงูู ุงููุธุงูุฉ ููู ุนู ุทุฑูู ุงูุฎุทุฃ ุนูู ุงูุซูุงุฌุฉ ุงูุชู ุญูุธุช ุจูุง.<LF>#ููุงุญ_ููุฑููุง,info_news,1'''))

# print(p.do_all('''ูุงุช 23 ุดุฎุต ูู ุงููุฑููุฌ ุจุนุฏ ุชููู ููุงุญ  #COVID19..<LF>ูููููุฑู ุจูุณุช ๐คฆ https://t.co/g51cmWeoVL,info_news,-1''', lemmatizor='farasapy'))
# print(p.do_all('''ูุงุช 23 ุดุฎุต ูู ุงููุฑููุฌ ุจุนุฏ ุชููู ููุงุญ  #COVID19..<LF>ูููููุฑู ุจูุณุช ๐คฆ https://t.co/g51cmWeoVL,info_news,-1''', lemmatizor='camel'))

# print(p.convert_emojis_to_meaning("๐๐ถ๐ถ๐คโค๐๐ป๐"));
# print("empty test");

