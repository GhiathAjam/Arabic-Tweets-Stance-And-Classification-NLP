import string
import re
import camel_tools.utils.dediac as camel_utils_dediac
import camel_tools.utils.normalize as camel_utils_normalize
import camel_tools.tokenizers.word as camel_utils_tokenizer
from camel_tools.disambig.mle import MLEDisambiguator
# from camel_tools.morphology.analyzer import Analyzer
# from camel_tools.morphology.database import MorphologyDB

import nltk
import arabicstopwords.arabicstopwords as stp

# SPECIAL TOKENS:
# <LINK> <NUM> <Mt> <LF> for links, numbers, mentions, line feed
# ENGLISH Text is kept as is
# HASHTAGS are repeated 3 times
# TODO: Emojis

class Preprocess:

    HASH_FREQ = 3
    mle = MLEDisambiguator.pretrained()

    nltk_arb_stopwords = set(nltk.corpus.stopwords.words("arabic"))
    arabicstopwords = set(stp.stopwords_list())

    STOPWORDS = arabicstopwords

    # def __init__(self, HASH_FREQ = 3) -> None:
    #     self.HASH_FREQ = HASH_FREQ
    #     self.mle = MLEDisambiguator.pretrained()
    #     self.STOPWORDS = self.arabicstopwords
    #     pass

    # string -> dediacritized string
    # print(dediac("Ù…ÙŠØ³Ø¨Øª"));
    def dediac(self, text):
        # Can also do using regex
        return camel_utils_dediac.dediac_ar(text)

    # replace:
        # 1. links with <LINK>
        # 2. numbers with <NUM>
        # 3. Mentions (@USER) with <Mt>
    def tokens(self, text):
        ret = text
        # all links are https://t.co/...
        # add extra space to avoid joining words
        ret = re.sub(r'http\S+', ' <LINK> ', ret)
        ret = re.sub(r'\d+', ' <NUM> ', ret)
        ret = re.sub(r'@\S+', ' <Mt> ', ret)
        ret = re.sub('<LF>', ' <LF> ', ret)
        return ret

    # string -> remove punctuation
    def remove_punctuation(self, text):
        punc = '''!()-[]{};:'"\,<>./?@#$%^&*_~''' + '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€Â«Â»'''
        # keep HashTags ?
        punc = punc.replace('#', '')
        punc = punc.replace('_', '')
        # keep tokens: <...>
        punc = punc.replace('<', '')
        punc = punc.replace('>', '')
        # replace punc with space
        return text.translate(str.maketrans(punc, ' ' * len(punc)))

    # replacing: Ø£ Ø¥ Ø¢ with Ø§
    # replacing: Ø© with Ù‡
    # replacing: ÙŠ with Ù‰
    # Maybe add Ø¦ Ø¤ ?
    def normalize(self, text):
        ret = text
        ret = camel_utils_normalize.normalize_alef_ar(ret)
        ret = camel_utils_normalize.normalize_teh_marbuta_ar(ret)
        ret = camel_utils_normalize.normalize_alef_maksura_ar(ret)
        # replace ØªØ¨Ø³Ù… with Øª Ø¨ Ø³ Ù… ?
        # ret = camel_utils_normalize.normalize_unicode(ret)
        return ret

    # English words?         --> kept intact
    # <LINK> <NUM> <Mt> ?    --> made into 3 tokens
    def tokenizer(self, text):
        # TODO: better handling of <LF> ??
        # TODO: check camel_tools.tokenizers.morphological.MorphologicalTokenizer

        # extract < .. >
        e1 = re.findall(r'<\S+>', text)
        # remove < .. >
        text = re.sub(r'<\S+>', '', text)
        # extact # .. _ .. _ ..
        e2 = re.findall(r'#(\S+)', text)
        # remove # .. _ .. _ ..
        text = re.sub(r'#\S+', '', text)

        ret = camel_utils_tokenizer.simple_word_tokenize(text)
        # add extracted!
        ret += e1
        # repeat hashtags for heavier weight!
        for tg in e2:
            ret += [tg] *self.HASH_FREQ

        return ret

    def camel_lemmatize(self, tokenized_text):
        # disambig = self.mle.disambiguate(tokenized_text)
        # lemmas = [d.analyses[0].analysis['lex'] for d in disambig]
        # return lemmas
        return tokenized_text

    def frasa_lemmatize(self, tokenized_text):
        return tokenized_text

    # Lemmatization
    def lemmatize(self, tokenized_text):
        # Camel
        cl = self.camel_lemmatize(tokenized_text)
        # Frasa
        fr = self.frasa_lemmatize(tokenized_text)
        
        # print(tokenized_text)
        # print("Camel: ", cl)
        # print("Frasa: ", fr)
        return tokenized_text

    # After lemmas?
    def remove_stopwords(self, tokenized_text):
        ret = [tk for tk in tokenized_text if tk not in self.STOPWORDS]
        return ret

    def do_all(self, text):
        return self.remove_stopwords(self.lemmatize(self.tokenizer(self.normalize(self.remove_punctuation(self.tokens(self.dediac(text)))))))


p = Preprocess()

# print(camel_utils_normalize.normalize_unicode('Ù…ÙŠØ³Ø¨Øª'))

# print(p.dediac('asd'))

# print(p.do_all('''Ø¨ÙŠÙ„ ØºÙŠØªØ³ ÙŠØªÙ„Ù‚Ù‰ Ù„Ù‚Ø§Ø­ #ÙƒÙˆÙÙŠØ¯19 Ù…Ù† ØºÙŠØ± ØªØµÙˆÙŠØ± Ø§Ù„Ø§Ø¨Ø±Ø© Ùˆ Ù„Ø§ Ø§Ù„Ø³ÙŠØ±Ù†Ø¬Ø© Ùˆ Ù„Ø§ Ø§Ù„Ø¯ÙˆØ§Ø¡ Ùˆ Ù„Ø§Ø¨Ø³ Ø¨ÙˆÙ„Ùˆ ØµÙŠÙÙŠ ÙÙŠ Ø¹Ø² Ø§Ù„Ø´ØªØ§Ø¡ Ùˆ ÙŠÙ‚ÙˆÙ„ Ø§Ù† Ø¥Ø­Ø¯Ù‰ Ù…Ø²Ø§ÙŠØ§ Ø¹Ù…Ø± Ø§Ù„ 65 Ø¹Ø§Ù…Ù‹Ø§ Ù‡ÙŠ Ø§Ù†Ù‡ Ù…Ø¤Ù‡Ù„ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø­ ... ÙŠØ¹Ù†Ù‰ Ù…Ø§ ÙƒØ§Ù† ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù„Ù‚Ø§Ø­ Ù„Ùˆ ÙƒØ§Ù† Ø¹Ù…Ø±Ù‡ Ø§ØµØºØ± Ù…Ù† 65 ğŸ¤” https://t.co/QQKFFUNwBn,celebrity,1ÙˆØ²ÙŠØ± Ø§Ù„ØµØ­Ø© Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ… ÙˆØªØ­Ø¯ÙŠØ¯Ø§ Ù‡Ù„Ø£ Ø¨Ù…Ø¤ØªÙ…Ø±ÙˆØ§ Ø§Ù„ØµØ­ÙÙŠ ÙƒØ§Ù† Ù…Ø§ Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„Ø© Ù…Ø¹ÙˆØ§.<LF>Ø¨Ø³ Ø§Ù†ÙˆØ§ Ø¨ÙØ§ÙŠØ²Ø± Ù‡Ùˆ Ø§Ù„Ù„Ù‚Ø§Ø­ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ø¢Ù…Ù† Ø¨Ø§Ù„Ø¹Ø§Ù„Ù… Ù„Ø£ Ø­Ø¨ÙŠØ¨ÙŠ Ù‡Ø§ÙŠ ÙÙŠÙ‡Ø§ Ù†ÙØ§Ù‚ ÙˆØ§Ø¶Ø­.<LF>Ø¹ÙÙˆØ§ ÙŠØ¹Ù†ÙŠ.<LF>Ù…Ø´ Ø¥Ù†ÙˆØ§ Ù…Ø¬Ø¨ÙˆØ±ÙŠÙ† ÙÙŠÙ‡ ÙˆÙ…Ø§ Ù‚Ø§Ø¯Ø± ØªØ¬Ø¨Ø± Ø§Ù„Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ø§Ø±Ù‚Ø© ØªØ¯ÙØ¹ Ù…ØµØ§Ø±ÙŠ ØªØ¬ÙŠØ¨ Ù„Ù‚Ø§Ø­ ØªØ§Ù†ÙŠ.<LF>ÙŠÙ„Ø§ Ø¨Ø§ÙŠ,info_news,1Ù‚ÙˆÙ„ÙƒÙ†  Ø±Ø­ ÙŠÙƒÙˆÙ†Ùˆ Ø§Ø¯ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø¨ Ù„Ø¨Ù†Ø§Ù† Ù„Ù…Ø§ ÙŠÙˆØµÙ„ Ø§Ù„Ù„Ù‚Ø§Ø­ØŸ<LF>Ø£Ù„ÙÙŠ Ø¬Ø±Ø¹Ø© Ù…Ù† Ù„Ù‚Ø§Ø­ #ÙƒÙˆØ±ÙˆÙ†Ø§ Ù„Ù„ØªÙ„Ù ÙÙŠ Ù…Ø³ØªØ´ÙÙ‰ Ø¨ÙˆØ³Ø·Ù†ØŒÙ„Ø£Ù† Ø¹Ø§Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§ÙØ© ÙˆÙ‚Ù Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø®Ø·Ø£ Ø¹Ù…Ù„ Ø§Ù„Ø«Ù„Ø§Ø¬Ø© Ø§Ù„ØªÙŠ Ø­ÙØ¸Øª Ø¨Ù‡Ø§.<LF>#Ù„Ù‚Ø§Ø­_ÙƒÙˆØ±ÙˆÙ†Ø§,info_news,1'''))

print(p.do_all('''Ù…Ø§Øª 23 Ø´Ø®Øµ ÙÙŠ Ø§Ù„Ù†Ø±ÙˆÙŠØ¬ Ø¨Ø¹Ø¯ ØªÙ„Ù‚ÙŠ Ù„Ù‚Ø§Ø­  #COVID19..<LF>Ù†ÙŠÙˆÙŠÙˆØ±Ùƒ Ø¨ÙˆØ³Øª ğŸ¤¦ https://t.co/g51cmWeoVL,info_news,-1'''))