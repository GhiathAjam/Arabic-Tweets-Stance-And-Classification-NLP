{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAtXrxn0wLHQ",
        "outputId": "363ff52a-fd8b-4455-e5d4-f8e3b2d5f62d"
      },
      "outputs": [],
      "source": [
        "!pip install camel-tools==1.2.0 farasapy nltk Arabic-Stopwords scikit-learn gensim pandas\n",
        "!camel_data light\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xFMDvRawLHU",
        "outputId": "16e09e88-4a9a-4504-f6be-969a0855c945"
      },
      "outputs": [],
      "source": [
        "# built in modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn import metrics \n",
        "\n",
        "# .py files \n",
        "from preprocess import Preprocess\n",
        "# from feature_extraction import *\n",
        "\n",
        "from classical_models import LSVMmodel\n",
        "# Can use this library to reload a specific module if the notebook can't see changes in the imported module\n",
        "import importlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mujwAMBwLHY"
      },
      "source": [
        "Stance detection labels meaning is as follows:\n",
        "1. Positive (1): means that the tweet author encourages and supports vaccination.\n",
        "2. Negative (-1): means that the tweet author refuses vaccination.\n",
        "3. Neutral (0): means that the tweet neither supports nor refuses vaccination.\n",
        "\n",
        "Category labels meaning is as follows:\n",
        "1. Info_News: Information about vaccination.\n",
        "2. Celebrities: mentioning celebrities taking vaccinations.\n",
        "3. Plan: Governmental plan or progress of vaccination.\n",
        "4. Request: Requests from governments regarding the vaccination process.\n",
        "5. Rumor: the tweet is a rumor.\n",
        "6. Advice: Advice related to the virus or the vaccination\n",
        "7. Restriction: Restrictions due to the virus e.g. traveling.\n",
        "8. Personal: Personal opinion or story about vaccination.\n",
        "9. Unrelated: Unrelated to vaccination.\n",
        "10.Others: Vaccination related but not one of the above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnUyQbwqwLHa"
      },
      "outputs": [],
      "source": [
        "# !unzip Dataset.zip\n",
        "t = pd.read_csv('./Dataset/train.csv')\n",
        "d = pd.read_csv('./Dataset/dev.csv')\n",
        "ts = pd.read_csv('./Dataset/test.csv')\n",
        "#\n",
        "# ov = pd.read_csv('/content/Dataset/classification_train_sample1.csv')\n",
        "# ov2 = pd.read_csv('/content/Dataset/cat_somesample.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psk40WbSwLHa"
      },
      "outputs": [],
      "source": [
        "# categories = [\"info_news\", \"celebrity\", \"plan\", \"requests\", \"rumors\", \"advice\", \"restrictions\", \"personal\", \"unrelated\", \"others\"]\n",
        "# def encode_category(y):\n",
        "#     '''\n",
        "#     Input: y a list of string labels for the category of each document \n",
        "#     Output: a list of encoded 10 sized array for the category of each doc \n",
        "#             for \"other\" category , it has an array =[0 0 0 0 0 0 0 0 0 1] \n",
        "#     '''\n",
        "#     return [categories.index(ele) for ele in y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN-cQ0ezwLHb"
      },
      "source": [
        "### Understanding the Data\n",
        "- 80% tweets are positive (class 1), the rest are neutral and negative.\n",
        "- 50% tweets belongs to info_news category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVr90KoqwLHb"
      },
      "outputs": [],
      "source": [
        "# # analyze dataset\n",
        "# print(\"counts for each lablel :\")\n",
        "# print(t['category'].value_counts(normalize=True))\n",
        "# print(ov['category'].value_counts(normalize=True))\n",
        "# print(ov2['category'].value_counts(normalize=True))\n",
        "# print(d['category'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxBYW9D9wLHb"
      },
      "outputs": [],
      "source": [
        "# print(\"####################################\")\n",
        "# print(\"duplicated rows :\")\n",
        "# print(t[t.duplicated()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VbngOOrwLHc"
      },
      "outputs": [],
      "source": [
        "X_train = t['text']\n",
        "# X_trainov = ov['text']\n",
        "# X_trainov2 = ov2['text']\n",
        "X_dev = d['text']\n",
        "X_test = ts['text']\n",
        "#\n",
        "Y_train = t['category']\n",
        "# Y_trainov = ov['category']\n",
        "# Y_trainov2 = ov2['category']\n",
        "Y_dev = d['category']\n",
        "# Y_test = ts['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfpneJGawLHc"
      },
      "source": [
        "#### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "8_tOO97iwLHc",
        "outputId": "98d05733-f774-4ea6-9ade-3b20194d5b22"
      },
      "outputs": [],
      "source": [
        "preprocess = Preprocess()\n",
        "X_train = X_train.apply(preprocess.do_all)\n",
        "# X_trainov = X_trainov.apply(preprocess.do_all)\n",
        "# X_trainov2 = X_trainov2.apply(preprocess.do_all)\n",
        "X_dev = X_dev.apply(preprocess.do_all)\n",
        "# X_test = X_test.apply(preprocess.do_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeh9AGOywLHd"
      },
      "outputs": [],
      "source": [
        "# r = random.randint(0, len(X_test))\n",
        "# # r = 900\n",
        "# print(r, '\\n')\n",
        "# print(ts.text[r], '\\n')\n",
        "# print(X_test[r], '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWM3pqe4wLHd"
      },
      "outputs": [],
      "source": [
        "# # Print Vocabulary size\n",
        "# lst = [word for x in X_train for word in x]\n",
        "# vocab = set(lst)\n",
        "# print(len(lst))\n",
        "# print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mgtuT2cwLHe"
      },
      "source": [
        "### Getting the train , test and dev features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_lb7u62wLHe"
      },
      "outputs": [],
      "source": [
        "# train_features_bow,test_features_bow=BOW(train_documents=X,test_documents=X_dev)\n",
        "# #\n",
        "# assert train_features_bow.shape[0] == len(X)\n",
        "# assert train_features_bow.shape[1] == len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzyuUgwdwLHe"
      },
      "outputs": [],
      "source": [
        "# train_features_tfidf,test_features_tfidf=TFIDF(train_documents=X,test_documents=X_dev)\n",
        "# #\n",
        "# assert train_features_tfidf.shape[0] == len(X)\n",
        "# assert train_features_tfidf.shape[1] == len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6k8I67QwLHe"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer =TfidfVectorizer(analyzer=lambda x: x)\n",
        "# train_features= vectorizer.fit_transform(X)\n",
        "# # return train_features.toarray(), test_features.toarray()\n",
        "\n",
        "# # print 10 words with highest tfidf values for each CATEGORY\n",
        "# feature_names = vectorizer.get_feature_names_out()\n",
        "# for i, category in enumerate(categories):\n",
        "#     lst = [0] *len(feature_names)\n",
        "#     for row in train_features[np.array(yc) == i].toarray():\n",
        "#         lst = [x + y for x, y in zip(lst, row)]\n",
        "#     # top 10 words\n",
        "#     top20 = np.argsort(lst)[::-1][:20]\n",
        "#     # print names and values\n",
        "#     print(category)\n",
        "#     print([(feature_names[j], lst[j]) for j in top20])\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea4FYrjcwLHf"
      },
      "outputs": [],
      "source": [
        "# train_features_cbow,test_features_cbow=CBOW(train_documents=X,test_documents=X_dev)\n",
        "# #\n",
        "# print(train_features_cbow.shape)\n",
        "# print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRHKvszBwLHf"
      },
      "outputs": [],
      "source": [
        "# train_features_sg,test_features_sg=SG(train_documents=X,test_documents=X_dev)\n",
        "# #\n",
        "# print(train_features_sg.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1kzMYg4wLHf"
      },
      "source": [
        "## Classical model Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYdn4HKAwLHg"
      },
      "source": [
        "- Classification Task Results:\n",
        "\n",
        "|  | SVM | Random Forest | KNN |  Voting system |\n",
        "| --------------- | --------------- | --------------- | --------------- | --------------- |\n",
        "| TFIDF |69%  |  |  ||\n",
        "| BOW | 54%  |   |   | |\n",
        "\n",
        "\n",
        "- Stance Task Results:\n",
        "\n",
        "|  | SVM | Random Forest | KNN |  Voting system |\n",
        "| --------------- | --------------- | --------------- | --------------- | --------------- |\n",
        "| TFIDF |81%  |  |  ||\n",
        "| BOW | 80%  |   |   | |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKczJpsywLHg"
      },
      "outputs": [],
      "source": [
        "# train_features = np.concatenate((train_features_bow, train_features_tfidf, train_features_cbow, train_features_sg), axis=1)\n",
        "# test_features = np.concatenate((test_features_bow, test_features_tfidf, test_features_cbow, test_features_sg), axis=1)\n",
        "# #\n",
        "# print(train_features_bow.shape)\n",
        "# print(train_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USifXj_ywLHg"
      },
      "outputs": [],
      "source": [
        "#  clf=LogisticRegression(max_iter=200, class_weight={0:1,1:2,2:3,3:4,4:1000,5:5,6:15,7:2,8:4,9:100000}\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def TFIDFW(train_documents_,test_documents_):\n",
        "    # vectorizer =TfidfVectorizer(analyzer=lambda x: x, ngram_range=(10, 10))\n",
        "    vectorizer =TfidfVectorizer(token_pattern=r'\\S+', lowercase=False, tokenizer=lambda x: x, ngram_range=(1, 3))\n",
        "    # vectorizer =TfidfVectorizer(analyzer='char', ngram_range=(4, 4))\n",
        "\n",
        "    train_documents = [\" \".join(doc) for doc in train_documents_]\n",
        "    test_documents = [\" \".join(doc) for doc in test_documents_]\n",
        "    train_features= vectorizer.fit_transform(train_documents)\n",
        "    test_features=vectorizer.transform(raw_documents=test_documents)\n",
        "    return train_features.toarray(), test_features.toarray()\n",
        "\n",
        "def TFIDFC(train_documents_,test_documents_):\n",
        "    # vectorizer =TfidfVectorizer(analyzer=lambda x: x, ngram_range=(10, 10))\n",
        "    # vectorizer =TfidfVectorizer(token_pattern=r'\\S+', lowercase=False, tokenizer=lambda x: x, ngram_range=(1, 4))\n",
        "    vectorizer =TfidfVectorizer(analyzer='char', ngram_range=(2, 3))\n",
        "\n",
        "    train_documents = [\" \".join(doc) for doc in train_documents_]\n",
        "    test_documents = [\" \".join(doc) for doc in test_documents_]\n",
        "    train_features= vectorizer.fit_transform(train_documents)\n",
        "    test_features=vectorizer.transform(raw_documents=test_documents)\n",
        "    return train_features.toarray(), test_features.toarray()\n",
        "\n",
        "def TFIDF(tr,ts):\n",
        "    trw, tsw = TFIDFW(tr, ts)\n",
        "    trc, tsc = TFIDFC(tr, ts)\n",
        "    return np.concatenate((trw, trc), axis=1), np.concatenate((tsw, tsc), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koGPvwYh2_2T",
        "outputId": "1710d48e-24d7-41f8-e70d-02a75e26978c"
      },
      "outputs": [],
      "source": [
        "# train_features, test_features = TFIDF(X_train, X_dev)\n",
        "\n",
        "# from sklearn import metrics\n",
        "\n",
        "# # clss = [NBmodel, KNNmodel, SVMmodel, LRmodel, RGmodel, RFmodel, DTmodel]\n",
        "# # clss = [KNNmodel]\n",
        "# clss = [LSVMmodel]\n",
        "# #\n",
        "# for cls in clss:\n",
        "#     #\n",
        "#     print(cls.__name__)\n",
        "#     yc_pred = cls(Xtrain=train_features, y_train=Y_train, X_test=test_features)\n",
        "#     # ys_pred = cls(Xtrain=train_features, y_train=Y_train['stance'], X_test=test_features)\n",
        "#     #\n",
        "#     print('========= Category =========')\n",
        "#     print(metrics.classification_report(y_true=Y_dev,y_pred=yc_pred))\n",
        "#     print('========= Stance =========')\n",
        "#     # print(metrics.classification_report(y_true=Y_dev['stance'],y_pred=ys_pred))\n",
        "\n",
        "# # classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_pred)\n",
        "# # stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmjFw6Of52dU",
        "outputId": "9356e8be-000f-49d5-a20e-388f086c1ef6"
      },
      "outputs": [],
      "source": [
        "# get output of test set\n",
        "X_train_dev = np.concatenate((X_train, X_dev), axis=0)\n",
        "Y_train_dev = np.concatenate((Y_train, Y_dev), axis=0)\n",
        "print(X_train.shape, X_dev.shape, X_train_dev.shape)\n",
        "print(Y_train_dev.shape)\n",
        "train_features, test_features = TFIDF(X_train_dev, X_test)\n",
        "\n",
        "yc_pred = LSVMmodel(Xtrain=train_features, y_train=Y_train_dev, X_test=test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD8u_6tm6_Rh",
        "outputId": "dccb94d5-1eb1-4c51-b3d0-a1d867ce424a"
      },
      "outputs": [],
      "source": [
        "# yc_pred\n",
        "pred = [(i, vi) for i, vi in enumerate(yc_pred)]\n",
        "\n",
        "print(pred[:15])\n",
        "# print(pred.value_counts())\n",
        "# print(metrics.classification_report(labels, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBXI2pG60xxH",
        "outputId": "40c8751e-515e-470d-8507-72864bd8dc6d"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "clf=svm.LinearSVC(class_weight='balanced')\n",
        "clf.fit(X=train_features,y=Y_train_dev)\n",
        "\n",
        "from joblib import dump, load\n",
        "dump(clf, 'filename.joblib') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlTBf9vC7Vjd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('./out/test_cat.csv', \"w\",encoding=\"utf-8\", newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(('id','stance'))\n",
        "    for row in pred:\n",
        "        writer.writerow(row)\n",
        "\n",
        "dd = pd.read_csv('./out/test_cat.csv')\n",
        "print(dd.head())\n",
        "\n",
        "print(dd.stance.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJB5b_IuwLHh"
      },
      "outputs": [],
      "source": [
        "asds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uj0y-4hwLHh"
      },
      "source": [
        "**SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc-L7djawLHh"
      },
      "outputs": [],
      "source": [
        "yc_pred=SVMmodel(Xtrain=train_features_bow,y_train=yc,X_test=test_features_bow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nh7DwZY-wLHi"
      },
      "outputs": [],
      "source": [
        "ys_pred=SVMmodel(Xtrain=train_features_tfidf,y_train=ys,X_test=test_features_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cieL1ZDwLHi"
      },
      "outputs": [],
      "source": [
        "yc_cbow_pred=SVMmodel(Xtrain=train_features_cbow,y_train=yc,X_test=test_features_cbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fih1dvvAwLHi"
      },
      "outputs": [],
      "source": [
        "ys_cbow_pred=SVMmodel(Xtrain=train_features_cbow,y_train=ys,X_test=test_features_cbow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ7DWcx8wLHi"
      },
      "outputs": [],
      "source": [
        "classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_cbow_pred)\n",
        "stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_cbow_pred)\n",
        "\n",
        "\n",
        "print(\"The classification accuracy after training on svm on all features : \",classify_accuracy)\n",
        "print(\"The stance accuracy after training on svm on all features : \",stance_accuracy)\n",
        "\n",
        "# F1 score\n",
        "print('========= Category =========')\n",
        "print(metrics.classification_report(y_true=yc_dev,y_pred=yc_cbow_pred))\n",
        "print('========= Stance =========')\n",
        "print(metrics.classification_report(y_true=ys_dev,y_pred=ys_cbow_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRUhTAHxwLHi"
      },
      "outputs": [],
      "source": [
        "yc_sg_pred=SVMmodel(Xtrain=train_features_sg,y_train=yc,X_test=test_features_sg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ImxURBwLHj"
      },
      "outputs": [],
      "source": [
        "ys_sg_pred=SVMmodel(Xtrain=train_features_sg,y_train=ys,X_test=test_features_sg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbTAefpCwLHj"
      },
      "outputs": [],
      "source": [
        "classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_sg_pred)\n",
        "stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_sg_pred)\n",
        "\n",
        "\n",
        "print(\"The classification accuracy after training on svm on all features : \",classify_accuracy)\n",
        "print(\"The stance accuracy after training on svm on all features : \",stance_accuracy)\n",
        "\n",
        "# F1 score\n",
        "print('========= Category =========')\n",
        "print(metrics.classification_report(y_true=yc_dev,y_pred=yc_sg_pred))\n",
        "print('========= Stance =========')\n",
        "print(metrics.classification_report(y_true=ys_dev,y_pred=ys_sg_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej-2WPNcwLHj"
      },
      "outputs": [],
      "source": [
        "# features: bow, tfidf, cbow\n",
        "yc_all_pred=SVMmodel(Xtrain=train_features,y_train=yc,X_test=test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G90BNy-cwLHj"
      },
      "outputs": [],
      "source": [
        "# features: bow, tfidf, cbow\n",
        "ys_all_pred=SVMmodel(Xtrain=train_features,y_train=ys,X_test=test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-NTkHnDwLHk"
      },
      "outputs": [],
      "source": [
        "# classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_pred)\n",
        "# stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_pred)\n",
        "\n",
        "# classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_cbow_pred)\n",
        "# stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_cbow_pred)\n",
        "\n",
        "classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_all_pred)\n",
        "stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_all_pred)\n",
        "\n",
        "\n",
        "print(\"The classification accuracy after training on svm on all features : \",classify_accuracy)\n",
        "print(\"The stance accuracy after training on svm on all features : \",stance_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCrL6GF0wLHk"
      },
      "outputs": [],
      "source": [
        "# F1 score\n",
        "print('========= Category =========')\n",
        "print(metrics.classification_report(y_true=yc_dev,y_pred=yc_all_pred))\n",
        "print('========= Stance =========')\n",
        "print(metrics.classification_report(y_true=ys_dev,y_pred=ys_all_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj_J-2xswLHk"
      },
      "outputs": [],
      "source": [
        "# features: bow, tfidf, cbow, sg\n",
        "yc_all_pred1=SVMmodel(Xtrain=train_features1,y_train=yc,X_test=test_features1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8bhkIFhwLHk"
      },
      "outputs": [],
      "source": [
        "# features: bow, tfidf, cbow, sg\n",
        "ys_all_pred1=SVMmodel(Xtrain=train_features1,y_train=ys,X_test=test_features1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iZW2LqrwLHk"
      },
      "outputs": [],
      "source": [
        "\n",
        "classify_accuracy1=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_all_pred1)\n",
        "stance_accuracy1=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_all_pred1)\n",
        "\n",
        "\n",
        "print(\"The classification accuracy after training on svm on all features : \",classify_accuracy1)\n",
        "print(\"The stance accuracy after training on svm on all features : \",stance_accuracy1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeOQv7OVwLHl"
      },
      "outputs": [],
      "source": [
        "# F1 score\n",
        "print('========= Category =========')\n",
        "print(metrics.classification_report(y_true=yc_dev,y_pred=yc_all_pred))\n",
        "print('========= Stance =========')\n",
        "print(metrics.classification_report(y_true=ys_dev,y_pred=ys_all_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN4QI2vGwLHl"
      },
      "outputs": [],
      "source": [
        "# F1 score\n",
        "print('========= Category =========')\n",
        "print(metrics.classification_report(y_true=yc_dev,y_pred=yc_all_pred))\n",
        "print('========= Stance =========')\n",
        "print(metrics.classification_report(y_true=ys_dev,y_pred=ys_all_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoDEXfstwLHl"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "# def initLSTM():\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 100 #need to be tuned\n",
        "hidden_size = 50 #need to be tuned\n",
        "\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "\n",
        "n_classes_category = 9\n",
        "n_classes_stance = 3\n",
        "\n",
        "linear_category = nn.Linear(hidden_size, n_classes_category)\n",
        "linear_stance = nn.Linear(hidden_size, n_classes_stance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH4MuLavwLHl"
      },
      "outputs": [],
      "source": [
        "def forward(self, sentences):\n",
        "    \"\"\"\n",
        "    This function does the forward pass of our model\n",
        "    Inputs:\n",
        "    - sentences: tensor of shape (batch_size, max_length)\n",
        "\n",
        "    Returns:\n",
        "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
        "    \"\"\"\n",
        "\n",
        "    final_output = None\n",
        "    ######################### TODO: implement the forward pass ####################################\n",
        "    # (1) Pass the sentences through the embedding layer\n",
        "    # (2) Pass the output of the embedding layer through the LSTM layer\n",
        "    # (3) Pass the output of the LSTM layer through the linear layer\n",
        "    # (4) Apply softmax to the output of the linear layer\n",
        "    # (5) Return the output of the softmax layer\n",
        "    sentences = embedding(sentences)\n",
        "    sentences, _ = lstm(sentences)\n",
        "    sentences_category = linear_category(sentences)\n",
        "    sentences_stance = linear_stance(sentences)\n",
        "    final_output_category = sentences_category\n",
        "    final_output_stance = sentences_stance\n",
        "    ###############################################################################################\n",
        "    return final_output_category, final_output_stance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAr-Pzi_wLHl"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV_HDniMwLHm"
      },
      "outputs": [],
      "source": [
        "# from keras.models import Sequential\n",
        "# MAX_SEQ_LEN = 1000\n",
        "\n",
        "# def model_1():\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = 128, input_length = MAX_SEQ_LEN))\n",
        "#     model.add(LSTM(128))\n",
        "#     model.add(Dense(64, activation='relu'))\n",
        "#     model.add(Dense(3, activation='softmax'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# m1 = train(model_1, \n",
        "#            train_text_vec,\n",
        "#            y_train,\n",
        "#            test_text_vec,\n",
        "#            y_test,\n",
        "#            checkpoint_path='model_1.h5',\n",
        "#            class_weights=cws\n",
        "#           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa_OggvswLHm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
