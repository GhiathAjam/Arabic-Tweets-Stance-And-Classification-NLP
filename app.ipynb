{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !camel_data light\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built in modules\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics \n",
    "\n",
    "# .py files \n",
    "from preprocess import Preprocess\n",
    "from feature_extraction import BOW\n",
    "from feature_extraction import TFIDF\n",
    "from feature_extraction import CBOW\n",
    "from feature_extraction import SG\n",
    "\n",
    "from classical_models import SVMmodel\n",
    "# Can use this library to reload a specific module if the notebook can't see changes in the imported module\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stance detection labels meaning is as follows:\n",
    "1. Positive (1): means that the tweet author encourages and supports vaccination.\n",
    "2. Negative (-1): means that the tweet author refuses vaccination.\n",
    "3. Neutral (0): means that the tweet neither supports nor refuses vaccination.\n",
    "\n",
    "Category labels meaning is as follows:\n",
    "1. Info_News: Information about vaccination.\n",
    "2. Celebrities: mentioning celebrities taking vaccinations.\n",
    "3. Plan: Governmental plan or progress of vaccination.\n",
    "4. Request: Requests from governments regarding the vaccination process.\n",
    "5. Rumor: the tweet is a rumor.\n",
    "6. Advice: Advice related to the virus or the vaccination\n",
    "7. Restriction: Restrictions due to the virus e.g. traveling.\n",
    "8. Personal: Personal opinion or story about vaccination.\n",
    "9. Unrelated: Unrelated to vaccination.\n",
    "10.Others: Vaccination related but not one of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('./Dataset/train.csv')\n",
    "d = pd.read_csv('./Dataset/dev.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis:  True\n",
      "Lemmatizor:  camel\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocess()\n",
    "#\n",
    "categories = [\"info_news\", \"celebrity\", \"plan\", \"requests\", \"rumors\", \"advice\", \"restrictions\", \"personal\", \"unrelated\", \"others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 9 categories so we have an array of len 9 for each y\n",
    "def encode_category(y):\n",
    "    '''\n",
    "    Input: y a list of string labels for the category of each document \n",
    "    Output: a list of encoded 10 sized array for the category of each doc \n",
    "            for \"other\" category , it has an array =[0 0 0 0 0 0 0 0 0 1] \n",
    "    '''\n",
    "    return [categories.index(ele) for ele in y]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data\n",
    "- 80% tweets are positive (class 1), the rest are neutral and negative.\n",
    "- 50% tweets belongs to info_news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6988 entries, 0 to 6987\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      6988 non-null   object\n",
      " 1   category  6988 non-null   object\n",
      " 2   stance    6988 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 163.9+ KB\n",
      "None\n",
      "####################################\n",
      "counts for each lablel :\n",
      "info_news       0.517459\n",
      "personal        0.146680\n",
      "celebrity       0.139525\n",
      "plan            0.086720\n",
      "unrelated       0.046222\n",
      "others          0.023898\n",
      "requests        0.016027\n",
      "rumors          0.011305\n",
      "advice          0.009588\n",
      "restrictions    0.002576\n",
      "Name: category, dtype: float64\n",
      "####################################\n",
      "counts for each stance label :\n",
      " 1    0.792501\n",
      " 0    0.144820\n",
      "-1    0.062679\n",
      "Name: stance, dtype: float64\n",
      " 1    0.804\n",
      " 0    0.126\n",
      "-1    0.070\n",
      "Name: stance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# analyze dataset\n",
    "print(t.info())\n",
    "# d\n",
    "# print(d.info())\n",
    "\n",
    "# count for each label\n",
    "print(\"####################################\")\n",
    "print(\"counts for each lablel :\")\n",
    "print(t['category'].value_counts(normalize=True))\n",
    "# same for d\n",
    "# print(d['category'].value_counts(normalize=True))\n",
    "\n",
    "# count for stance labels\n",
    "print(\"####################################\")\n",
    "print(\"counts for each stance label :\")\n",
    "print(t['stance'].value_counts(normalize=True))\n",
    "# d\n",
    "print(d['stance'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t['text']\n",
    "ys = t['stance']\n",
    "yc = t['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "duplicated rows :\n",
      "                                                   text   category  stance\n",
      "529   اللقاح لازم ينحفظ بدرجة حرارة ٧٠ دون الصفر وال...  info_news       1\n",
      "740   هناك مصادر لقاح أثبتت نجاحها غير أمريكا ويمكن ...  info_news       1\n",
      "963   لو كان اللقاح الأمريكي أمن ومفيد لما أصبحت الو...   personal      -1\n",
      "977   اللقاح الأمريكي لم ينجح في الاختبارات السريرية...  info_news      -1\n",
      "1684    #نريد_لقاح_آمن حتى نأمن صحة اطفالنا في المستقبل   requests       1\n",
      "1878  هناك مصادر لقاح أثبتت نجاحها غير أمريكا ويمكن ...  info_news       1\n",
      "2477  محمد بن زايد: الإمارات قدمت أكثر من مليون و 27...  info_news       1\n",
      "2644         نريد التخلص من كورونا لكننا #نريد_لقاح_آمن   requests       1\n",
      "2646  في ظل تسابق دول العالم على تطعيم شعوبها.. تَعَ...  info_news       1\n",
      "2786  لو كان اللقاح الأمريكي أمن ومفيد لما أصبحت الو...   personal      -1\n",
      "3231  #نريد_لقاح_آمن حتى ترجع الحياة الى طبيعتها بعد...  info_news       1\n",
      "3728    #نريد_لقاح_آمن حتى نأمن صحة اطفالنا في المستقبل   requests       1\n",
      "3801  #نريد_لقاح_آمن حتى ترجع الحياة الى طبيعتها بعد...   requests       1\n",
      "3889         نريد التخلص من كورونا لكننا #نريد_لقاح_آمن  info_news       1\n",
      "3910  امريكا تريد سد عجز أزمتها المالية الخانقة ببيع...  info_news       1\n",
      "4380    #نريد_لقاح_آمن حتى نأمن صحة اطفالنا في المستقبل   requests       1\n",
      "4429        اللقاح الأمريكي غالي ومو آمن #نريد_لقاح_آمن  info_news       1\n",
      "4438  #نريد_لقاح_آمن حتى ترجع الحياة الى طبيعتها بعد...   requests       1\n",
      "4458  اللقاح الأمريكي مفروض على العراق فرضاً<LF>#نري...  info_news       1\n",
      "4473  الذي تفنن بقتلنا لا يؤتمن لكي نأخذ منه اللقاح ...     others       1\n",
      "4576  دعبول حضرتك منو انت وتطلب من قائد دولة إسلامية...  info_news       1\n",
      "4682        اللقاح الأمريكي غالي ومو آمن #نريد_لقاح_آمن  info_news       1\n",
      "4844  #عاجل| #وزير_الصحة_التركي: بدء تطعيم المواطنين...  info_news       1\n",
      "5007  ما سبب الإصرار على اللقاح الأمريكي يوجد إنه في...  info_news      -1\n",
      "5010  تعمد عملاء الداخل يردون فرضه علينا <LF>هذه الف...  info_news       1\n",
      "5033  #نريد_لقاح_آمن حتى ترجع الحياة الى طبيعتها بعد...   personal       1\n",
      "5317  هناك مصادر لقاح أثبتت نجاحها غير أمريكا ويمكن ...  info_news       1\n",
      "5417         نريد التخلص من كورونا لكننا #نريد_لقاح_آمن   personal       1\n",
      "5436  اللقاح الأمريكي لم ينجح في الاختبارات السريرية...  info_news      -1\n",
      "5480  الخائن لا يؤتمن فكيف اذا كانت امريكا هي الخائن...   requests       1\n",
      "5765  روسيا تعلن استعدادها لتزويد مصر بتكنولوجيا تصن...  info_news       1\n",
      "5956  هناك مصادر لقاح أثبتت نجاحها غير أمريكا ويمكن ...  info_news       1\n",
      "5984  هناك مصادر لقاح أثبتت نجاحها غير أمريكا ويمكن ...  info_news       1\n",
      "6029  الذي تفنن بقتلنا لا يؤتمن لكي نأخذ منه اللقاح ...   personal      -1\n",
      "6135             محتاجين لقاح يقاوم الإفراط في التفكير.  unrelated       0\n",
      "6514  #نريد_لقاح_آمن حتى ترجع الحياة الى طبيعتها بعد...   requests       1\n",
      "6858  رئيس لجنة الصحة النيابية النائب عاصم عراجي: طل...       plan       1\n",
      "6899                       نبي لقاح ضد التفكير الزايد .  unrelated       0\n"
     ]
    }
   ],
   "source": [
    "print(\"####################################\")\n",
    "print(\"duplicated rows :\")\n",
    "print(t[t.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = t['text']\n",
    "ys = t['stance']\n",
    "yc = t['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    celebrity\n",
      "1    info_news\n",
      "2    info_news\n",
      "3    celebrity\n",
      "4     personal\n",
      "5    info_news\n",
      "6    info_news\n",
      "7     personal\n",
      "8    unrelated\n",
      "9    info_news\n",
      "Name: category, dtype: object\n",
      "[1, 0, 0, 1, 7, 0, 0, 7, 8, 0]\n"
     ]
    }
   ],
   "source": [
    "print(yc[0:10])\n",
    "yc=encode_category(yc)\n",
    "print(yc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=d['text']\n",
    "ys_dev=d['stance']\n",
    "yc_dev=d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_dev=encode_category(yc_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data\n",
    "X = X.apply(preprocess.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dev data\n",
    "X_dev = X_dev.apply(preprocess.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6916 \n",
      "\n",
      "السديس عن تلقي الملك سلمان لقاح كورونا: قدم للشعب السعودي والعالم درسا  <LF>https://t.co/wImO0WR13q \n",
      "\n",
      "['السديس', 'عَن', 'أَلْقَى', 'مَلِك', 'سَلْمان', 'لَقاح', 'كَوَّر', 'قَدَّم', 'شَعْب', 'سَعُودِيّ', 'عالَم', 'دَرْس', '<LF>', '<LINK>'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = random.randint(0, len(X))\n",
    "# r = 900\n",
    "print(r, '\\n')\n",
    "print(t.text[r], '\\n')\n",
    "print(X[r], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 \n",
      "\n",
      "تساؤلات آخر ليل <LF>اذا ما كفانا لقاح الكورونا ...بزيدولو مي ؟؟ 😅<LF>#هبل \n",
      "\n",
      "['تَساؤُل', 'آخَر', 'كَفَى', 'لَقاح', 'الكورونا', 'بزيدولو', 'مَيّ', '<LF>', '<smiling_face_with_open_mouth_&_cold_sweat>', '<LF>', 'أَهْبَل', 'أَهْبَل'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = random.randint(0, len(X_dev))\n",
    "r = 150\n",
    "# r = 738\n",
    "print(r, '\\n')\n",
    "print(d.text[r], '\\n')\n",
    "print(X_dev[r], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157990\n",
      "11755\n"
     ]
    }
   ],
   "source": [
    "# Print Vocabulary size\n",
    "lst = [word for x in X for word in x]\n",
    "vocab = set(lst)\n",
    "print(len(lst))\n",
    "print(len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the train , test and dev features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_bow,test_features_bow=BOW(train_documents=X,test_documents=X_dev)\n",
    "#\n",
    "assert train_features_bow.shape[0] == len(X)\n",
    "assert train_features_bow.shape[1] == len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tfidf,test_features_tfidf=TFIDF(train_documents=X,test_documents=X_dev)\n",
    "#\n",
    "assert train_features_tfidf.shape[0] == len(X)\n",
    "assert train_features_tfidf.shape[1] == len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info_news\n",
      "[('<LF>', 294.27850738746736), ('كَوَّر', 226.6223101860665), ('لَقاح', 216.47097715595743), ('<NUM>', 201.26844710071643), ('<LINK>', 177.23343555817593), ('جُرْعَة', 86.49775703928718), ('فَيْرُوس', 83.84257450743797), ('عاجِل', 82.22749840409844), ('صِحَّة', 81.42622511965635), ('تَطْعِيم', 77.1925725400968), ('فايزر', 75.3925624882043), ('كوفيد', 60.67509055645215), ('مِلْيُون', 58.68444790998813), ('عَرَبِيّ', 56.742524008525024), ('ضِدّ', 55.955724898757246), ('دَوْلَة', 52.39212299688869), ('سَعُودِيّ', 52.091068021707), ('أَمارَة', 50.663568829629476), ('يَوْم', 48.48224587671623), ('أَراد', 46.33697390658954)]\n",
      "\n",
      "celebrity\n",
      "[('تَلَقَّى', 116.90073580612544), ('<LF>', 96.44052474656165), ('كَوَّر', 92.25816784582415), ('مَلِك', 80.05493094970217), ('خادِم', 77.22553708457893), ('حَرَم', 76.63791147518536), ('شَرِيف', 70.9978018609423), ('جُرْعَة', 69.98564104042319), ('<LINK>', 69.63818048288577), ('لَقاح', 67.28260845292984), ('سَلْمان', 41.43832195114347), ('اللَّه', 35.02221688161321), ('كوفيد', 34.75066545738224), ('أَمِير', 34.48348332805764), ('حِفْظ', 29.76496821377269), ('وَزِير', 25.637885063810263), ('<NUM>', 25.250171341305546), ('سَعُودِيّ', 25.19018021598461), ('عَبْدالعَزِيز', 24.820027105090368), ('عاجِل', 24.300524636979514)]\n",
      "\n",
      "plan\n",
      "[('<LF>', 44.543281949775725), ('كَوَّر', 44.16436957329216), ('<NUM>', 38.68463765511055), ('لَقاح', 33.80502591509359), ('<LINK>', 28.885808158086252), ('صِحَّة', 24.251047930188435), ('تَطْعِيم', 23.35797947670318), ('مِلْيُون', 19.736029246911855), ('جُرْعَة', 18.809300097735385), ('عاجِل', 17.936174678968897), ('فَيْرُوس', 16.829458390223348), ('أَمارَة', 13.371749886490681), ('وَزِير', 13.13975937208669), ('يَوْم', 12.736569772691869), ('ضِدّ', 12.601311637310625), ('أَعْلَن', 11.496690458184213), ('كوفيد', 11.363505643168295), ('أَلْف', 11.320528802283508), ('دَوْلَة', 11.232222960438204), ('عَرَبِيّ', 10.578629176518099)]\n",
      "\n",
      "requests\n",
      "[('أَراد', 21.729710367069263), ('لَقاح', 10.137053326428063), ('<LF>', 6.018548326006069), ('كَوَّر', 3.5074794125918545), ('أَوْقَف', 2.9513543524320442), ('طَبِيعَة', 2.930352384465371), ('شَيّ', 2.899453353549001), ('الفايروس', 2.7490389609248496), ('حَياة', 2.7418369299662606), ('مُسْتَقْبَل', 2.6915858087533384), ('<NUM>', 2.555971544034251), ('طِفْل', 2.516888676408385), ('<LINK>', 2.436299543966477), ('صِحَّة', 2.201929729889873), ('تَطْعِيم', 2.134720967437606), ('إِسْرائِيل', 2.1337993926346845), ('قاتِل', 1.859626763111061), ('أَمْرِيكِيّ', 1.7955311645767746), ('أَمْرِيكا', 1.7825843792090137), ('فِلَسْطِينِيّ', 1.7154695523186088)]\n",
      "\n",
      "rumors\n",
      "[('<LF>', 7.127487828997555), ('لَقاح', 5.2747087841022715), ('كَوَّر', 4.790192903606551), ('<LINK>', 3.0022457986942284), ('إِشاعَة', 2.5714800972452627), ('<face_with_tears_of_joy>', 2.5610453404826203), ('صِحَّة', 2.505166291795888), ('مُتَحَدِّث', 1.6103558660511423), ('شائِع', 1.5949493400429664), ('مُؤامَرَة', 1.46650097390892), ('<NUM>', 1.466136212085877), ('<rolling_on_the_floor_laughing>', 1.457949266248713), ('وِزارَة', 1.399821804972832), ('<backhand_index_pointing_down>', 1.3758488513367326), ('نَظَرِيَّة', 1.3498451221736003), ('كوفيد', 1.2902872243207608), ('أَبْرَز', 1.268429661199531), ('مَشّ', 1.2649791135892476), ('مَعْلُومَة', 1.2594233903484404), ('رَدّ', 1.2154223365085997)]\n",
      "\n",
      "advice\n",
      "[('<LF>', 6.539433545261279), ('كَوَّر', 4.883778792794592), ('لَقاح', 4.237927458076373), ('<LINK>', 4.115137130348363), ('صِحَّة', 2.55980354035126), ('تَطْعِيم', 2.453454126585852), ('وَجَب', 2.202276393689326), ('<NUM>', 1.8935477818201034), ('أَلْقَى', 1.854152191429135), ('كوفيد', 1.8485793455088322), ('جُرْعَة', 1.7809755368282238), ('خَبِير', 1.3531738349385658), ('<syringe>', 1.3232362581766604), ('أَمْكَن', 1.2948198874326737), ('فَيْرُوس', 1.2635761461171375), ('حُصُول', 1.226066998210837), ('قِيام', 1.207167019440059), ('عَرَف', 1.1979716296433631), ('يَوْم', 1.1969623444170616), ('تَطْبِيق', 1.1941114400229786)]\n",
      "\n",
      "restrictions\n",
      "[('سَفَر', 2.4484145841292215), ('<LF>', 2.2387673663071843), ('<NUM>', 1.4010683973661995), ('شَرْط', 1.386229062267983), ('جَواز', 1.3220699384245376), ('شَهادَة', 1.2881603451637633), ('كَوَّر', 1.2489746895787024), ('لَقاح', 1.1098253218190641), ('سَماح', 1.0811041170432563), ('جُرْعَة', 0.8877427741917944), ('كوفيد', 0.8681161491646845), ('خارِج', 0.8135259253232314), ('صِحِّيّ', 0.8131001601129608), ('صِحَّة', 0.7885861860251333), ('<LINK>', 0.761689581035269), ('بَقِي', 0.7415786446419406), ('فاوتشي', 0.7080342239879253), ('<Mt>', 0.7005269858402317), ('سافَر', 0.6974171588817122), ('حُصُول', 0.6742739035458123)]\n",
      "\n",
      "personal\n",
      "[('<LF>', 81.894626253632), ('لَقاح', 63.129432916551735), ('اللَّه', 47.20484866433475), ('كَوَّر', 41.16606841400818), ('شُكْر', 34.261708402223356), ('<NUM>', 31.517218966799142), ('<LINK>', 30.698825867079087), ('أَراد', 29.410223311155157), ('جُرْعَة', 28.989888791613264), ('كوفيد', 27.367695492914343), ('صِحَّة', 25.519038682793543), ('حَمْد', 23.156101718570284), ('يَوْم', 22.734680263017328), ('تَطْعِيم', 21.500491342909896), ('ثانِي', 20.390421273131622), ('مُواطِن', 17.377362373320267), ('<Mt>', 17.153548323923328), ('تَنْظِيم', 16.39224686343813), ('وِزارَة', 16.001636250082388), ('تَلَقَّى', 14.86100151291806)]\n",
      "\n",
      "unrelated\n",
      "[('<LF>', 23.658768321799823), ('لَقاح', 15.77531639002766), ('تَفْكِير', 8.353936714595717), ('<NUM>', 8.027219174979809), ('<LINK>', 7.758207358750362), ('ضِدّ', 6.965871053549326), ('أَراد', 5.846261977291153), ('نَبِيّ', 5.787288428183294), ('تَطْعِيم', 5.329835631227171), ('<Mt>', 5.170017603347587), ('زايِد', 4.838847414413291), ('قاوَم', 4.540244469483882), ('مُحْتاج', 4.409797370712048), ('كَوَّر', 4.063979705326059), ('مَرَض', 4.020100315254508), ('ٱِحْتاج', 3.839211382506741), ('ناس', 3.6631493349990376), ('إِفْراط', 3.6115489786326957), ('️', 3.4694972882302006), ('لُبْنان', 3.399952076798069)]\n",
      "\n",
      "others\n",
      "[('<LF>', 11.29315983406956), ('لَقاح', 11.170758626551237), ('كَوَّر', 7.722342158910509), ('أَراد', 5.692367092503103), ('<LINK>', 5.472044391254545), ('<face_with_tears_of_joy>', 5.047163832461507), ('<NUM>', 4.488849753107799), ('<Mt>', 2.9329726113026466), ('اللَّه', 2.6883368201962776), ('سَعُودِيّ', 2.53692600576294), ('تَطْعِيم', 2.4881157487125254), ('عالَم', 2.17433316782312), ('كوفيد', 2.1485349727464635), ('فَيْرُوس', 1.7953225159768795), ('لُبْنانِيّ', 1.756261603004744), ('عَذاب', 1.7291178743408233), ('صِحَّة', 1.618949481142853), ('عَمّ', 1.6000896988924111), ('حَرِيرِيّ', 1.580448216062056), ('عَلِم', 1.5069375139017251)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer =TfidfVectorizer(analyzer=lambda x: x)\n",
    "train_features= vectorizer.fit_transform(X)\n",
    "# return train_features.toarray(), test_features.toarray()\n",
    "\n",
    "# print 10 words with highest tfidf values for each CATEGORY\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for i, category in enumerate(categories):\n",
    "    lst = [0] *len(feature_names)\n",
    "    for row in train_features[np.array(yc) == i].toarray():\n",
    "        lst = [x + y for x, y in zip(lst, row)]\n",
    "    # top 10 words\n",
    "    top20 = np.argsort(lst)[::-1][:20]\n",
    "    # print names and values\n",
    "    print(category)\n",
    "    print([(feature_names[j], lst[j]) for j in top20])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tfidf,test_features_cbow=CBOW(train_documents=X,test_documents=X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tfidf,test_features_sg=SG(train_documents=X,test_documents=X_dev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical model Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification Task Results:\n",
    "\n",
    "|  | SVM | Random Forest | KNN |  Voting system |\n",
    "| --------------- | --------------- | --------------- | --------------- | --------------- |\n",
    "| TFIDF |69%  |  |  ||\n",
    "| BOW | 54%  |   |   | |\n",
    "\n",
    "\n",
    "- Stance Task Results:\n",
    "\n",
    "|  | SVM | Random Forest | KNN |  Voting system |\n",
    "| --------------- | --------------- | --------------- | --------------- | --------------- |\n",
    "| TFIDF |81%  |  |  ||\n",
    "| BOW | 80%  |   |   | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6988, 11755)\n",
      "(6988, 23510)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.concatenate((train_features_bow, train_features_tfidf), axis=1)\n",
    "test_features = np.concatenate((test_features_bow, test_features_tfidf), axis=1)\n",
    "#\n",
    "print(train_features_bow.shape)\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Category =========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73       545\n",
      "           1       0.85      0.81      0.83       145\n",
      "           2       0.22      0.16      0.19        82\n",
      "           3       0.33      0.10      0.15        20\n",
      "           4       0.00      0.00      0.00        15\n",
      "           5       0.50      0.10      0.17        10\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.47      0.51      0.49       128\n",
      "           8       0.50      0.42      0.45        36\n",
      "           9       0.12      0.06      0.08        17\n",
      "\n",
      "    accuracy                           0.64      1000\n",
      "   macro avg       0.37      0.29      0.31      1000\n",
      "weighted avg       0.61      0.64      0.62      1000\n",
      "\n",
      "========= Stance =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# clss = [NBmodel, KNNmodelو SVMmodel]\n",
    "# clss = [KNNmodel]\n",
    "clss = [LRmodel]\n",
    "#\n",
    "for cls in clss:\n",
    "    #\n",
    "    N = 1000\n",
    "    N = len(train_features)\n",
    "    yc_pred = cls(Xtrain=train_features[:N], y_train=yc[:N], X_test=test_features)\n",
    "    # ys_pred = cls(Xtrain=train_features, y_train=ys, X_test=test_features)\n",
    "    #\n",
    "    print('========= Category =========')\n",
    "    print(metrics.classification_report(y_true=yc_dev,y_pred=yc_pred))\n",
    "    print('========= Stance =========')\n",
    "    # print(metrics.classification_report(y_true=ys_dev,y_pred=ys_pred))\n",
    "\n",
    "# classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_pred)\n",
    "# stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc_pred=SVMmodel(Xtrain=train_features_bow,y_train=yc,X_test=test_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred=SVMmodel(Xtrain=train_features_bow,y_train=ys,X_test=test_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_accuracy=metrics.accuracy_score(y_true=yc_dev,y_pred=yc_pred)\n",
    "stance_accuracy=metrics.accuracy_score(y_true=ys_dev,y_pred=ys_pred)\n",
    "\n",
    "print(\"The classification accuracy after training on svm on bow features : \",classify_accuracy)\n",
    "print(\"The stance accuracy after training on svm on bow features : \",stance_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d93ce000fb5f8fcca2dab3e75c99c50bb371be07ecadf3a0d8d467af599caa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
