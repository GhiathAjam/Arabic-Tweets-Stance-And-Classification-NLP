{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model  (BOW)\n",
    "# represent all the bags in a matrix of size (number of bags, number of words in the vocabulary) \n",
    "# each row is a bag and each column is a word in the vocabulary\n",
    "# the value in each cell is the number of times the word appears in the bag\n",
    "# the vocabulary is the set of all the words in the corpus\n",
    "# the corpus is the set of all the bags\n",
    "# the number of bags is the number of documents in the corpus\n",
    "# the number of words in the vocabulary is the number of unique words in the corpus\n",
    "\n",
    "\n",
    "# TF-IDF model\n",
    "# Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis:  True\n",
      "Lemmatizor:  camel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from preprocess import Preprocess \n",
    "preprocess = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "t = pd.read_csv('./Dataset/train.csv')\n",
    "X = t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the preprocessing\n",
    "X = X.apply(preprocess.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بِيل\n"
     ]
    }
   ],
   "source": [
    "print(X[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ghieath\\Desktop\\D77\\Projects\\Arabic-Tweets-Stance-And-Classification-NLP\\Feature extraction.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# count vectorizer function takes sentences as input \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# convert it into matrix representation \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# where each cell will be filled by the frequency of each vocab\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bow_model \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# count vectorizer function takes sentences as input \n",
    "# convert it into matrix representation \n",
    "# where each cell will be filled by the frequency of each vocab\n",
    "# vectorizer = CountVectorizer()\n",
    "# bow_model = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1780</th>\n",
       "      <th>65</th>\n",
       "      <th>9kte81z1jk</th>\n",
       "      <th>co</th>\n",
       "      <th>https</th>\n",
       "      <th>lf</th>\n",
       "      <th>nnatra5bpb</th>\n",
       "      <th>qbustq1kpy</th>\n",
       "      <th>qqkffunwbn</th>\n",
       "      <th>user</th>\n",
       "      <th>...</th>\n",
       "      <th>يشتم</th>\n",
       "      <th>يطلع</th>\n",
       "      <th>يعنى</th>\n",
       "      <th>يعني</th>\n",
       "      <th>يقول</th>\n",
       "      <th>يكونو</th>\n",
       "      <th>يلا</th>\n",
       "      <th>يلي</th>\n",
       "      <th>يوصل</th>\n",
       "      <th>١٦</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1780  65  9kte81z1jk  co  https  lf  nnatra5bpb  qbustq1kpy  qqkffunwbn  \\\n",
       "0     0   2           0   1      1   0           0           0           1   \n",
       "1     0   0           0   0      0   4           0           0           0   \n",
       "2     0   0           0   0      0   2           0           0           0   \n",
       "3     0   0           0   1      1   0           0           1           0   \n",
       "4     0   0           0   0      0   1           0           0           0   \n",
       "5     0   0           0   1      1   1           1           0           0   \n",
       "6     0   0           0   0      0   0           0           0           0   \n",
       "7     0   0           0   0      0   2           0           0           0   \n",
       "8     1   0           1   1      1   4           0           0           0   \n",
       "9     0   0           0   0      0   1           0           0           0   \n",
       "\n",
       "   user  ...  يشتم  يطلع  يعنى  يعني  يقول  يكونو  يلا  يلي  يوصل  ١٦  \n",
       "0     0  ...     0     0     1     0     1      0    0    0     0   0  \n",
       "1     0  ...     0     0     0     1     0      0    1    0     0   0  \n",
       "2     0  ...     0     0     0     0     0      1    0    0     1   0  \n",
       "3     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "4     1  ...     1     0     0     0     0      0    0    0     0   0  \n",
       "5     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "6     0  ...     0     0     0     0     0      0    0    1     0   1  \n",
       "7     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "8     1  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "9     0  ...     0     1     0     0     0      0    0    0     0   0  \n",
       "\n",
       "[10 rows x 227 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the matrx into pandas dataframe \n",
    "# to set the column names as actual vocab words which are our features\n",
    "# df_bow = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())\n",
    "# df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1780', '65', '9kte81z1jk', 'co', 'https', 'lf', 'nnatra5bpb', 'qbustq1kpy', 'qqkffunwbn', 'user', 'ألفي', 'أميركا', 'أنقرة', 'أوكسفام', 'أول', 'إحدى', 'إسلامية', 'إنوا', 'اتابع', 'اد', 'اذا', 'اشترى', 'اصغر', 'اعلامية', 'ال', 'الآمن', 'الأجدر', 'الإنفوغراف', 'الابرة', 'الاخبار', 'التوزيع', 'التي', 'الثلاجة', 'الخطأ', 'الخليجية', 'الدواء', 'الدول', 'الدولة', 'الدين', 'السيرنجة', 'الشتاء', 'الصحة', 'الصحفي', 'الصيني', 'الضريبة', 'العادل', 'اللقاح', 'المارقة', 'المسؤولية', 'النظافة', 'الوحيد', 'الى', 'اليوم', 'امريكي', 'ان', 'انت', 'انه', 'انوا', 'اول', 'بالعاصمة', 'بالعالم', 'باي', 'بدعم', 'بدها', 'بدهم', 'بس', 'بعتقد', 'بعض', 'بفايزر', 'بفضل', 'بكرا', 'بكل', 'بمؤتمروا', 'بنذكر', 'بها', 'بوسطن', 'بولو', 'بيل', 'تاني', 'تجبر', 'تجيب', 'تحت', 'تدعي', 'تدفع', 'تركيا', 'تستحي', 'تصوير', 'تصير', 'تهكير', 'توقيفه', 'ثرواتهم', 'جاء', 'جرعة', 'حبيبي', 'حسابك', 'حضرتك', 'حفظت', 'دبي', 'دعبول', 'دعبول_دومه_مسحول', 'دولة', 'رح', 'رضت', 'سنة', 'شئت', 'شلنا', 'شي', 'صيفي', 'طريق', 'طلة', 'عاصي_الحلاني', 'عام', 'عامل', 'عاملين', 'عتويتر', 'عز', 'عشوا', 'عفوا', 'على', 'عليهم', 'عمر', 'عمره', 'عمرهم', 'عمل', 'عن', 'عندي', 'عوارض', 'غيتس', 'غير', 'فافعل', 'فخر', 'في', 'فيروس_اللامساواة', 'فيه', 'فيها', 'قائد', 'قادر', 'قريبا', 'قصة', 'قلق', 'قوجة', 'قولكن', 'كان', 'كل', 'كورونا', 'كورونافاك', 'كوفيد19', 'لأ', 'لأن', 'لا', 'لابس', 'لاخذ', 'لانه', 'لبنان', 'لبنان_ينتفض', 'لحد', 'لعد', 'لقاح', 'لقاح_الناس', 'لقاح_كورونا', 'للتلف', 'للحصول', 'لم', 'لما', 'لو', 'مؤامرة', 'مؤسسة', 'مؤهل', 'ما', 'مابدهم', 'ماذا', 'متزايد', 'متفوقين', 'مجالاتهم', 'مجبورين', 'محاربة_الفقر', 'محاربة_اللامساواة', 'مزايا', 'مستشفى', 'مش', 'مشكلة', 'مصاري', 'معوا', 'مفكرينها', 'مليونان', 'مليونيرا', 'من', 'منو', 'مين', 'نج', 'نفاق', 'هاي', 'هذا', 'هلأ', 'هم', 'هناك', 'هو', 'هي', 'وأخذ', 'وئام', 'واضح', 'والناس', 'واليوم', 'وتحديدا', 'وتطلب', 'وراح', 'وزير', 'وطرده', 'وقف', 'وما', 'وهاب', 'ويتسافه', 'ويلي', 'ياخذ', 'ياخذوا', 'يتلقى', 'يحتاج', 'يشتم', 'يطلع', 'يعنى', 'يعني', 'يقول', 'يكونو', 'يلا', 'يلي', 'يوصل', '١٦']\n"
     ]
    }
   ],
   "source": [
    "#show all features (unique words)\n",
    "# print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec is trained to predict the nearest word belongs to the context\n",
    "# e.g. to tell if \"milk\" is a likely word given the \"The cat was drinking...\"\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=11771, vector_size=100, alpha=0.025>\n",
      "['<LF>', 'لَقاح', 'كَوَّر', '<LINK>', 'مِن']\n",
      "[ 0.15797299  1.0271599   0.2459128   0.38740587 -0.19596358 -1.0615919\n",
      "  1.1133935   1.4800682   0.08663098 -0.97659045 -0.38142172 -0.13453768\n",
      " -0.08024624 -0.11601485  0.11494155 -0.62216854 -0.5156574   0.01009275\n",
      " -0.6197641  -1.5965697   0.46110302  0.22279264  0.63213915  0.39160424\n",
      " -0.70412606  0.02154753 -0.8511575   0.20015326 -0.1642272   0.18242268\n",
      "  0.38068435 -1.3028823   0.94843    -0.8127714   0.13824067  0.5523272\n",
      "  0.4260973  -0.12407244 -0.465209   -0.28427893  0.18546472 -0.928797\n",
      " -0.55697864  0.08220238  0.7302846  -0.5922981  -0.20359166  0.32068416\n",
      "  0.42363027  0.2685378   1.3292007  -0.9939686  -0.54295266 -1.212226\n",
      " -0.84041727  0.3780009   0.33251625  0.03234111 -0.04726193 -0.4514139\n",
      " -0.6711979   0.00943678  0.51292586  0.18704778 -1.1114746   1.4529542\n",
      "  1.059956    0.79214865 -1.4766232   1.5296981  -0.5786286   0.53255725\n",
      "  1.3785952   0.08550846  0.38774678  0.01566987 -0.5210425   0.4002163\n",
      " -0.56775886  0.3012125  -1.2935572  -0.0754697  -0.7580671   0.97522104\n",
      "  0.36888108 -0.4095549   0.3596768   0.26614138  0.05513028 -0.28642422\n",
      "  1.0409915  -0.2819417   0.02901497 -0.8217512   0.9321622  -0.2203132\n",
      "  1.3904009  -0.5147914   0.45785636 -0.05112927]\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW (Continuous Bag of Words) model \n",
    "model_cbow = Word2Vec(X, min_count = 1, vector_size = 100, window = 5) \n",
    "#applied on the preprocessed data X\n",
    "print(model_cbow)\n",
    "words_cbow = list(model_cbow.wv.index_to_key)\n",
    "print(words_cbow[0:5])\n",
    "\n",
    "# print the vector of a word\n",
    "print(model_cbow.wv['مِن'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=11771, vector_size=100, alpha=0.025>\n",
      "['<LF>', 'لَقاح', 'كَوَّر', '<LINK>', 'مِن']\n",
      "[-0.04610743  0.11783698 -0.18406776  0.2401241   0.249771   -0.5030298\n",
      "  0.47608382  0.36614117 -0.24949072 -0.33939964 -0.50018424 -0.2328751\n",
      " -0.10511613  0.0108497   0.27510035 -0.12743714 -0.08238885 -0.330762\n",
      " -0.26623264 -0.61349136  0.35966766  0.25545105  0.38874513  0.07432896\n",
      " -0.04484693  0.05048626 -0.10526902  0.14291842 -0.42703727 -0.03194794\n",
      "  0.1614878  -0.57182443  0.29483104 -0.55971074  0.20683017  0.0890462\n",
      " -0.14058107 -0.03294833  0.04239803 -0.12583943  0.42319068 -0.6815682\n",
      " -0.43984142  0.30274537  0.37155274 -0.04900031 -0.14009161 -0.03959699\n",
      "  0.10912527  0.08577565  0.3030974  -0.38825122  0.22929136 -0.29469794\n",
      "  0.19323115 -0.14233823  0.24137078  0.09268752  0.4223588  -0.11875439\n",
      " -0.05434862 -0.24375202  0.06677754  0.05906361 -0.19014166  0.4158615\n",
      "  0.4597643   0.39117315 -0.47909775  0.40424755 -0.24015482  0.01999854\n",
      "  0.6201087  -0.00453458  0.07972143 -0.15925351 -0.5441861  -0.03629855\n",
      " -0.03159988 -0.39768037 -0.54619616  0.03725811 -0.22456287  0.1023812\n",
      " -0.11749247 -0.04997608  0.23424876 -0.09330586 -0.3523645  -0.13335733\n",
      "  0.47591463 -0.23493731  0.00074498 -0.1354427   0.18892199  0.11664232\n",
      "  0.42391205 -0.22110514 -0.03895488  0.1776795 ]\n"
     ]
    }
   ],
   "source": [
    "model_sg = Word2Vec(X, min_count = 1, vector_size = 100, window = 5, sg=1) \n",
    "#applied on the preprocessed data X\n",
    "print(model_sg)\n",
    "words_cbow = list(model_sg.wv.index_to_key)\n",
    "print(words_cbow[0:5])\n",
    "\n",
    "# print the vector of a word\n",
    "print(model_sg.wv['مِن'])\n",
    "\n",
    "# model_sg.wv['word']: has the vector of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
