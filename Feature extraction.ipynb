{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words model  (BOW)\n",
    "# represent all the bags in a matrix of size (number of bags, number of words in the vocabulary) \n",
    "# each row is a bag and each column is a word in the vocabulary\n",
    "# the value in each cell is the number of times the word appears in the bag\n",
    "# the vocabulary is the set of all the words in the corpus\n",
    "# the corpus is the set of all the bags\n",
    "# the number of bags is the number of documents in the corpus\n",
    "# the number of words in the vocabulary is the number of unique words in the corpus\n",
    "\n",
    "\n",
    "# TF-IDF model\n",
    "# Word2Vec model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis:  True\n",
      "Lemmatizor:  camel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from preprocess import Preprocess \n",
    "preprocess = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "t = pd.read_csv('./Dataset/train.csv')\n",
    "X = t['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the preprocessing\n",
    "X = X.apply(preprocess.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بِيل', 'غيتس', 'تَلَقَّى', 'لَقاح', 'مِن', 'غَيْر', 'تَصْوِير', 'إِبْرَة', 'وَ', 'السيرنجه', 'وَ', 'دَواء', 'وَ', 'لابَس', 'بُولُو', 'صَيْفِيّ', 'فِي', 'عِزّ', 'شِتاء', 'وَ', 'قال', 'أَنَّ', 'أَحَد', 'مَزِيَّة', 'عُمَر', 'ال', 'عام', 'هِيَ', 'أَنَّ', 'مُؤَهَّل', 'حُصُول', 'عَلَى', 'لَقاح', 'عَنَى', 'ٱِحْتاج', 'لَقاح', 'لَو', 'عُمْر', 'أَصْغَر', 'مِن', '<NUM>', '<NUM>', '<NUM>', '<thinking_face>', '<LINK>', 'كوفيد', 'كوفيد', 'كوفيد']\n",
      "['وَزِير', 'صِحَّة', 'لَحَد', 'يَوْم', 'تَحْدِيد', 'هَلّا', 'بمؤتمروا', 'صُحُفِيّ', 'عِنْد', 'مُشْكِلَة', 'مَعا', 'بَسّ', 'آن', 'بفايزر', 'هُوَ', 'لَقاح', 'وَحِيد', 'أَمْن', 'عالَم', 'حُبَيْبِيّ', 'هاي', 'فِي', 'أَفاق', 'واضِح', 'عَفْو', 'عَنَى', 'مَشّ', 'آن', 'مَجْبُور', 'فِي', 'قادِر', 'أَجْبَر', 'دَوْلَة', 'مارِق', 'دَفَع', 'مَصارِيّ', 'أَجاب', 'لَقاح', 'تَأَنِّي', 'يلا', 'أَيّ', '<LF>', '<LF>', '<LF>', '<LF>']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ghieath\\Desktop\\D77\\Projects\\Arabic-Tweets-Stance-And-Classification-NLP\\Feature extraction.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# count vectorizer function takes sentences as input \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# convert it into matrix representation \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# where each cell will be filled by the frequency of each vocab\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ghieath/Desktop/D77/Projects/Arabic-Tweets-Stance-And-Classification-NLP/Feature%20extraction.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m bow_model \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(X)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# count vectorizer function takes sentences as input \n",
    "# convert it into matrix representation \n",
    "# where each cell will be filled by the frequency of each vocab\n",
    "# vectorizer = CountVectorizer()\n",
    "# bow_model = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# print(bow_model.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1780</th>\n",
       "      <th>65</th>\n",
       "      <th>9kte81z1jk</th>\n",
       "      <th>co</th>\n",
       "      <th>https</th>\n",
       "      <th>lf</th>\n",
       "      <th>nnatra5bpb</th>\n",
       "      <th>qbustq1kpy</th>\n",
       "      <th>qqkffunwbn</th>\n",
       "      <th>user</th>\n",
       "      <th>...</th>\n",
       "      <th>يشتم</th>\n",
       "      <th>يطلع</th>\n",
       "      <th>يعنى</th>\n",
       "      <th>يعني</th>\n",
       "      <th>يقول</th>\n",
       "      <th>يكونو</th>\n",
       "      <th>يلا</th>\n",
       "      <th>يلي</th>\n",
       "      <th>يوصل</th>\n",
       "      <th>١٦</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1780  65  9kte81z1jk  co  https  lf  nnatra5bpb  qbustq1kpy  qqkffunwbn  \\\n",
       "0     0   2           0   1      1   0           0           0           1   \n",
       "1     0   0           0   0      0   4           0           0           0   \n",
       "2     0   0           0   0      0   2           0           0           0   \n",
       "3     0   0           0   1      1   0           0           1           0   \n",
       "4     0   0           0   0      0   1           0           0           0   \n",
       "5     0   0           0   1      1   1           1           0           0   \n",
       "6     0   0           0   0      0   0           0           0           0   \n",
       "7     0   0           0   0      0   2           0           0           0   \n",
       "8     1   0           1   1      1   4           0           0           0   \n",
       "9     0   0           0   0      0   1           0           0           0   \n",
       "\n",
       "   user  ...  يشتم  يطلع  يعنى  يعني  يقول  يكونو  يلا  يلي  يوصل  ١٦  \n",
       "0     0  ...     0     0     1     0     1      0    0    0     0   0  \n",
       "1     0  ...     0     0     0     1     0      0    1    0     0   0  \n",
       "2     0  ...     0     0     0     0     0      1    0    0     1   0  \n",
       "3     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "4     1  ...     1     0     0     0     0      0    0    0     0   0  \n",
       "5     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "6     0  ...     0     0     0     0     0      0    0    1     0   1  \n",
       "7     0  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "8     1  ...     0     0     0     0     0      0    0    0     0   0  \n",
       "9     0  ...     0     1     0     0     0      0    0    0     0   0  \n",
       "\n",
       "[10 rows x 227 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the matrx into pandas dataframe \n",
    "# to set the column names as actual vocab words which are our features\n",
    "# df_bow = pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())\n",
    "# df_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1780', '65', '9kte81z1jk', 'co', 'https', 'lf', 'nnatra5bpb', 'qbustq1kpy', 'qqkffunwbn', 'user', 'ألفي', 'أميركا', 'أنقرة', 'أوكسفام', 'أول', 'إحدى', 'إسلامية', 'إنوا', 'اتابع', 'اد', 'اذا', 'اشترى', 'اصغر', 'اعلامية', 'ال', 'الآمن', 'الأجدر', 'الإنفوغراف', 'الابرة', 'الاخبار', 'التوزيع', 'التي', 'الثلاجة', 'الخطأ', 'الخليجية', 'الدواء', 'الدول', 'الدولة', 'الدين', 'السيرنجة', 'الشتاء', 'الصحة', 'الصحفي', 'الصيني', 'الضريبة', 'العادل', 'اللقاح', 'المارقة', 'المسؤولية', 'النظافة', 'الوحيد', 'الى', 'اليوم', 'امريكي', 'ان', 'انت', 'انه', 'انوا', 'اول', 'بالعاصمة', 'بالعالم', 'باي', 'بدعم', 'بدها', 'بدهم', 'بس', 'بعتقد', 'بعض', 'بفايزر', 'بفضل', 'بكرا', 'بكل', 'بمؤتمروا', 'بنذكر', 'بها', 'بوسطن', 'بولو', 'بيل', 'تاني', 'تجبر', 'تجيب', 'تحت', 'تدعي', 'تدفع', 'تركيا', 'تستحي', 'تصوير', 'تصير', 'تهكير', 'توقيفه', 'ثرواتهم', 'جاء', 'جرعة', 'حبيبي', 'حسابك', 'حضرتك', 'حفظت', 'دبي', 'دعبول', 'دعبول_دومه_مسحول', 'دولة', 'رح', 'رضت', 'سنة', 'شئت', 'شلنا', 'شي', 'صيفي', 'طريق', 'طلة', 'عاصي_الحلاني', 'عام', 'عامل', 'عاملين', 'عتويتر', 'عز', 'عشوا', 'عفوا', 'على', 'عليهم', 'عمر', 'عمره', 'عمرهم', 'عمل', 'عن', 'عندي', 'عوارض', 'غيتس', 'غير', 'فافعل', 'فخر', 'في', 'فيروس_اللامساواة', 'فيه', 'فيها', 'قائد', 'قادر', 'قريبا', 'قصة', 'قلق', 'قوجة', 'قولكن', 'كان', 'كل', 'كورونا', 'كورونافاك', 'كوفيد19', 'لأ', 'لأن', 'لا', 'لابس', 'لاخذ', 'لانه', 'لبنان', 'لبنان_ينتفض', 'لحد', 'لعد', 'لقاح', 'لقاح_الناس', 'لقاح_كورونا', 'للتلف', 'للحصول', 'لم', 'لما', 'لو', 'مؤامرة', 'مؤسسة', 'مؤهل', 'ما', 'مابدهم', 'ماذا', 'متزايد', 'متفوقين', 'مجالاتهم', 'مجبورين', 'محاربة_الفقر', 'محاربة_اللامساواة', 'مزايا', 'مستشفى', 'مش', 'مشكلة', 'مصاري', 'معوا', 'مفكرينها', 'مليونان', 'مليونيرا', 'من', 'منو', 'مين', 'نج', 'نفاق', 'هاي', 'هذا', 'هلأ', 'هم', 'هناك', 'هو', 'هي', 'وأخذ', 'وئام', 'واضح', 'والناس', 'واليوم', 'وتحديدا', 'وتطلب', 'وراح', 'وزير', 'وطرده', 'وقف', 'وما', 'وهاب', 'ويتسافه', 'ويلي', 'ياخذ', 'ياخذوا', 'يتلقى', 'يحتاج', 'يشتم', 'يطلع', 'يعنى', 'يعني', 'يقول', 'يكونو', 'يلا', 'يلي', 'يوصل', '١٦']\n"
     ]
    }
   ],
   "source": [
    "#show all features (unique words)\n",
    "# print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec is trained to predict the nearest word belongs to the context\n",
    "# e.g. to tell if \"milk\" is a likely word given the \"The cat was drinking...\"\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW (Continuous Bag of Words) model \n",
    "model_cbow = Word2Vec(X, min_count = 1, vector_size = 300, window = 5) \n",
    "# applied on the preprocessed data X\n",
    "# print(model_cbow)\n",
    "# words_cbow = list(model_cbow.wv.index_to_key)\n",
    "# print(words_cbow[0:5])\n",
    "\n",
    "# print the vector of a word\n",
    "# print(model_cbow.wv.get_vector('مِن'))\n",
    "# print(len(model_cbow.wv[model_cbow.wv.index_to_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create skip-gram model (SG) \n",
    "model_sg = Word2Vec(X, min_count = 1, vector_size = 300, window = 5, sg=2) \n",
    "# applied on the preprocessed data X\n",
    "# print(model_sg)\n",
    "# words_sg = list(model_sg.wv.index_to_key)\n",
    "# print(words_sg[0:5])\n",
    "\n",
    "# print the vector of a word\n",
    "# print(model_sg.wv['مِن'])\n",
    "\n",
    "# model_sg.wv['word']: has the vector of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't apply mean for the LSTM/RNN models\n",
    "def get_mean_vector(word2vec_model, words):\n",
    "    # remove out-of-vocabulary words\n",
    "    words = [word for word in words if word in word2vec_model.wv.index_to_key]\n",
    "    if len(words) >= 1:\n",
    "        return np.mean(word2vec_model.wv[words], axis=0)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vector of the first sentence\n",
    "get_mean_vector(model_cbow, X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_vector(model_sg, X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append the vectors of all sentences to a list\n",
    "vectors_cbow = []\n",
    "vectors_sg = []\n",
    "# get the vector of all sentences \n",
    "for sentence in X:\n",
    "    vec = get_mean_vector(model_cbow, sentence)\n",
    "    if len(vec) > 0:\n",
    "        vectors_cbow.append(vec)\n",
    "\n",
    "for sentence in X:\n",
    "    vec = get_mean_vector(model_sg, sentence)\n",
    "    if len(vec) > 0:\n",
    "        vectors_sg.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
