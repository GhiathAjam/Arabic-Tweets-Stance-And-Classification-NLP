{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\farasa\\__base.py:157: UserWarning: You are using old version of java. Farasa is compatiple with Java 7 and above \n",
      "  warnings.warn(\n",
      "[2022-12-26 14:18:07,643 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "[2022-12-26 14:18:10,468 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "c:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# built in modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "\n",
    "# .py files \n",
    "from preprocess import Preprocess\n",
    "from feature_extraction import TFIDF\n",
    "from feature_extraction import SGLstm\n",
    "\n",
    "# Can use this library to reload a specific module if the notebook can't see changes in the imported module\n",
    "import importlib\n",
    "from torch import nn\n",
    "from keras.models import Sequential\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('./Dataset/train.csv')\n",
    "d = pd.read_csv('./Dataset/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emojis:  False\n",
      "Lemmatizor:  farasapy\n"
     ]
    }
   ],
   "source": [
    "preprocess = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"info_news\", \"celebrity\", \"plan\", \"requests\", \"rumors\", \"advice\", \"restrictions\", \"personal\", \"unrelated\", \"others\"]\n",
    "\n",
    "stance = [\"0\", \"1\", \"2\"] # 0: -ve, 1: 0, 2: +ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 9 categories so we have an array of len 9 for each y\n",
    "# def encode_category(y):\n",
    "#     '''\n",
    "#     Input: y a list of string labels for the category of each document \n",
    "#     Output: a list of encoded 10 sized array for the category of each doc \n",
    "#             for \"other\" category , it has an array =[0 0 0 0 0 0 0 0 0 1] \n",
    "#     '''\n",
    "#     return [categories.index(ele) for ele in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we have 9 categories so we have an array of len 9 for each y\n",
    "# def encode_stance(y):\n",
    "#     '''\n",
    "#     Input: y a list of string labels for the category of each document \n",
    "#     Output: a list of encoded 10 sized array for the category of each doc \n",
    "#             for \"other\" category , it has an array =[0 0 0 0 0 0 0 0 0 1] \n",
    "#     ''' \n",
    "#     return [stance.index(ele) for ele in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_beforeTokenization = t['text']\n",
    "ys = t['stance']\n",
    "yc = t['category']\n",
    "\n",
    "X_beforeTokenization_dev = d['text']\n",
    "ys_dev = d['stance']\n",
    "yc_dev = d['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yc=encode_category(yc)\n",
    "\n",
    "# do I need this?\n",
    "# ys=encode_stance(ys)\n",
    "\n",
    "# yc_dev=encode_category(yc_dev)\n",
    "# ys_dev=encode_stance(ys_dev)\n",
    "\n",
    "# Preprocess the test data\n",
    "X = X_beforeTokenization.apply(preprocess.do_all)\n",
    "X_dev = X_beforeTokenization_dev.apply(preprocess.do_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649553\n",
      "16526\n"
     ]
    }
   ],
   "source": [
    "# all words in the dataset \n",
    "lst = [word for x in X for word in x]\n",
    "print(len(lst))\n",
    "\n",
    "# unique words\n",
    "vocab = set(lst)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "# sg_model = KeyedVectors.load_word2vec_format('./Word2VecSkipGram300D.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n"
     ]
    }
   ],
   "source": [
    "max_length=max([len(s) for s in X])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SG features\n",
    "train_features_sg,test_features_sg=SGLstm(train_documents=X,test_documents=X_dev,max_len=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6988\n",
      "(126, 300)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features_sg))\n",
    "print(np.shape(train_features_sg[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itf-idf features\n",
    "# train_features_tfidf,test_features_tfidf=TFIDF(train_documents=X,test_documents=X_dev)\n",
    "# print(train_features_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply sequential model, need to pass inputs of the same size to the model \n",
    "# so we need to pad the input to the same size\n",
    "# we will pad the input to the maximum length of the input\n",
    "\n",
    "# get the maximum length of the input\n",
    "max_length=max([len(s) for s in train_features_sg])\n",
    "print(max_length)\n",
    "print(len(train_features_sg[0][0]))\n",
    "\n",
    "train_features_sg_arr= np.zeros(len(train_features_sg))\n",
    "# All sentences should be padded to have the same length\n",
    "for i in range(len(train_features_sg)):\n",
    "    train_features_sg_arr[i]=train_features_sg[i]+['<PAD>'] * (max_length - len(X[i]))\n",
    "    # train_features_sg[i][0].extend(['<PAD>'] * (max_length - len(X[i])))\n",
    "# print([ys[0]])\n",
    "# ys_list=[]\n",
    "# ys_list.append(ys)\n",
    "# ys_list[0].extend([0] * 1)\n",
    "# ys_list[1].extend([0] * 1)\n",
    "# print(ys_list)\n",
    "# for i in range(len(ys)):\n",
    "#     ys[i].extend([0] * (max_length - len(ys[i])))\n",
    "# for i in range(len(yc)):\n",
    "#     yc[i].extend([0] * (max_length - len(yc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# word vector\n",
    "print(len(train_features_sg[0][0]))\n",
    "print(train_features_sg_arr[0])\n",
    "print(len(train_features_sg_arr[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_features_sg))\n",
    "print(train_features_sg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be 3D\n",
    "# print(train_features_sg.shape)\n",
    "# # number of tweets, length of each tweet, number of features \n",
    "# train_features_sg_3d= np.reshape(train_features_sg, (len(train_features_sg), train_features_sg.shape[1], 1))\n",
    "# print(train_features_sg_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def initLSTM():\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300 #need to be tuned\n",
    "hidden_size = 50 #need to be tuned\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "n_classes_category = 9\n",
    "n_classes_stance = 3\n",
    "\n",
    "linear_category = nn.Linear(hidden_size, n_classes_category)\n",
    "linear_stance = nn.Linear(hidden_size, n_classes_stance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = torch.nn.LSTM(input_size=300, hidden_size=50, num_layers=1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m lstm_model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m128\u001b[39m, input_shape\u001b[39m=\u001b[39m(max_length, \u001b[39m300\u001b[39m), return_sequences\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n\u001b[0;32m      4\u001b[0m lstm_model\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.2\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m lstm_model\u001b[39m.\u001b[39;49madd(LSTM(\u001b[39m64\u001b[39;49m))\n\u001b[0;32m      6\u001b[0m lstm_model\u001b[39m.\u001b[39madd(Dropout(\u001b[39m0.2\u001b[39m))\n\u001b[0;32m      7\u001b[0m lstm_model\u001b[39m.\u001b[39madd(Dense(\u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:227\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_explicit_input_shape \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m    225\u001b[0m   \u001b[39m# If the model is being built continuously on top of an input layer:\u001b[39;00m\n\u001b[0;32m    226\u001b[0m   \u001b[39m# refresh its output.\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m   output_tensor \u001b[39m=\u001b[39m layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    228\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(nest\u001b[39m.\u001b[39mflatten(output_tensor)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m _standardize_args(inputs,\n\u001b[0;32m    663\u001b[0m                                                      initial_state,\n\u001b[0;32m    664\u001b[0m                                                      constants,\n\u001b[0;32m    665\u001b[0m                                                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants)\n\u001b[0;32m    667\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 668\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    670\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    674\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:983\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called on\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[39m# symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\u001b[39mself\u001b[39m, inputs, args, kwargs, input_list):\n\u001b[1;32m--> 983\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m    984\u001b[0m                                             input_list)\n\u001b[0;32m    986\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m    987\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1121\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1118\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   1119\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value):\n\u001b[0;32m   1120\u001b[0m   \u001b[39m# Check input assumptions set after layer building, e.g. input shape.\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m   outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[0;32m   1122\u001b[0m       inputs, input_masks, args, kwargs)\n\u001b[0;32m   1124\u001b[0m   \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1126\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1127\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m).\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:854\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature)\n\u001b[0;32m    853\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 854\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(inputs, args, kwargs, input_masks)\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:892\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_scope()):  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    887\u001b[0m   \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m    888\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m    889\u001b[0m     \u001b[39m# Build layer if applicable (if the `build` method has been\u001b[39;00m\n\u001b[0;32m    890\u001b[0m     \u001b[39m# overridden).\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     \u001b[39m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_build(inputs)\n\u001b[0;32m    893\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[0;32m    894\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:2628\u001b[0m, in \u001b[0;36mLayer._maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_maybe_build\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m   2626\u001b[0m   \u001b[39m# Check input assumptions set before layer building, e.g. input rank.\u001b[39;00m\n\u001b[0;32m   2627\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m-> 2628\u001b[0m     input_spec\u001b[39m.\u001b[39;49massert_input_compatibility(\n\u001b[0;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_spec, inputs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   2630\u001b[0m     input_list \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[0;32m   2631\u001b[0m     \u001b[39mif\u001b[39;00m input_list \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype_policy\u001b[39m.\u001b[39mcompute_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\emada\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    213\u001b[0m   ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[0;32m    214\u001b[0m   \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[1;32m--> 215\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(input_index) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m of layer \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    216\u001b[0m                      layer_name \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m is incompatible with the layer: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    217\u001b[0m                      \u001b[39m'\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(spec\u001b[39m.\u001b[39mndim) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, found ndim=\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    218\u001b[0m                      \u001b[39mstr\u001b[39m(ndim) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m. Full shape received: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m\n\u001b[0;32m    219\u001b[0m                      \u001b[39mstr\u001b[39m(\u001b[39mtuple\u001b[39m(shape)))\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m   ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 128)"
     ]
    }
   ],
   "source": [
    "# Build the model for stances\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(128, input_shape=(max_length, 300), return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(64))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(3, activation='softmax'))\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm_model.summary())\n",
    "ys = np.array(ys) + 1\n",
    "\n",
    "# Fit the lstm_model\n",
    "history = lstm_model.fit(train_features_sg, ys, epochs=12, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lstm_model.predict(test_features_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F1 score for each class\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 score for each class -> \",f1_score(ys, y_pred.argmax(axis=1), average=None))\n",
    "# Calculate the Macro Average F1 score for the whole data\n",
    "print(\"Macro Average F1 score -> \",f1_score(ys, y_pred.argmax(axis=1), average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d93ce000fb5f8fcca2dab3e75c99c50bb371be07ecadf3a0d8d467af599caa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
